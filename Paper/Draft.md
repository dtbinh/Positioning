# Draft

_[Abstract: ... ]_

<!--TOC-->


# 1. INTRODUCTION

{{Introduction.md}}


# 2. LITERATURE REVIEW

{{Literature review.md}}


# 3. METHODOLOGY

…[^computation]

[^computation]: All the models in this paper are build in MATLAB (R2015a) and run on a MacBook Pro with a 2.6 GHz quad-core Intel Core i7 processor. In addition several models use MATLAB’s Parallel Computing Toolbox with 4 local workers taking advantage of the quad-core processor structure to process several runs in parallel.

### Consumers

The scope of this paper is limited to one side of the market -- the firms and their decision-making behaviour. For this reason simplifying assumptions are used with regard to the other side of the market -- the consumers and the purchasing behaviour of consumers. For one thing consumers are static. They do not shift position, they maintain preferences, they are obliques to trends, and are in no way influenced by changes in the market. Secondly consumers behave non-strategically and demand inelastically. They always purchase a single unit from the closest firm, regardless of the distance to the firm. And finally all firms are identical in the eyes of the consumer. There is no brand recognition and no loyalty -- the customer purchase from the closest firm regardless of previous purchasing history.

Every consumer has an ideal point in the market. This is the point where the consumer wants a firm to locate. All other points are second to the ideal point. And the further away from the ideal point that the firm locates, the less attractive it becomes. Consumers have *single peaked preferences*. Essentially this paper reduces consumers to being merely their ideal points. 

Viewed in terms of geography the ideal point would be the consumer's physical location and the distance is the bee-line distance to firms. Alternatively the model can be viewed in terms of product differentiation. Then the ideal point of the consumer is his or her desired product characteristic along the two dimensions. We assume *horizontal product differentiation* along both dimension, ie. product characteristic are non-ordinal. In addition we need to assume that product characteristics are continuous and infinitely divisible.

The utility function for each consumer is a function of the distance from the consumer’s ideal point and to the position of the firm. The utility function is equivalent to a quadratic loss function. The Euclidean distance metric is used. The utility of customer $i$ is the negative squared distance between customer $i$ and firm $j$:

$$ U_i(j) = -d(i,j)^2 $$

#### Distribution of customers

The above described simplification of consumer behaviour allows focused attention on the behaviour of firms. As discussed in the literature review the distribution of consumers can fundamentally change the behaviour of firms. We would like to capture and analyse these changes -- in particular the effects from asymmetric and multimodal distributions. We follow the method used by Laver and Sergenti (2011) and assume two consumer subpopulations in our market. When the two subpopulations have the same mean ideal point then the aggregated distribution of consumers will be symmetric and unimodal. While subpopulations with vastly different mean ideal points and different sizes will lead to an aggregated distribution of consumers that is asymmetric and bimodal. Thus this method of using two subpopulations is able to capture several different types of aggregate distributions using only two parameters; the polarisation of subpopulation ideal points, and the relative size of the two subpopulations. 

The market is two-dimensional, so we use the two-dimensional equivalent of the *normal distribution* -- the *bivariate normal distribution*. Both subpopulations follow a bivariate normal distribution. Using different mean values in the distribution imply that the subpopulations disagree over the ideal point. Without further restrictions the subpopulations would disagree on both dimensions. To simplify the analysis, and without lose of generality, we assume that the subpopulations have a common ideal point along one dimension and only disagree over the ideal point along the other dimension. This is equivalent to rotating the market space or coordinate system until disagreement only appear along one of the dimensions. Ie. rotating the market space such that the line going through the mean ideal points of subpopulations is parallel to an axis. One can always rotate the coordinate system without loss of information, and similarly this simplifies our analysis without any loss of generality. This is similar to the orthogonal transformation underlying a *principal component analysis*. We start out with two dimensions or components, and end up with two new dimensions; the disagreement dimension (x-axis) and the agreement dimension (y-axis). The two subpopulations -- from now on referred to as 'left' and 'right' -- follow a *bivariate normal distribution* with mean $(-\mu,0)$ and mean $(\mu,0)$ respectively, and both with standard deviation (0.5, 0.5).[^covarmatrix]. Where the parameter $\mu$ measures the ideal point polarisation. The parameter measuring the relative size of the left subpopulation is $n_l/n_r$. The aggregated consumer distribution is a *mixed bivariate normal distribution* with weights based on the relative size of the subpopulations.

The mean ideal point on the y-axis is 0 for both subpopulations hence no disagreement along this dimension. The mean ideal point on the x-axis is $-\mu$ for the left subpopulation and $\mu$ for the right subpopulation. For $\mu > 0$ there is a disagreement along the x-axis. I will analysis the polarisation parameter in the range $\mu \in [0, 1.5]$ -- a range that spans both a unified and a split market. At the lower bound of the range there is no disagreement and the market consists of one unified customer base. At the upper bound there is essentially no overlap between the subpopulations and the customer base is split. With no overlap we are at the edge of what constitutes a single market[^singlemarket], and it might be more appropriate to describe this setting as two separate markets, at least in terms of the customers. The range of the relative subpopulation size is $n_l/n_r \in [1, 2]$. The subpopulations are equally large when the $n_l/n_r = 1$, while $n_l/n_r = 2$ indicates that the left subpopulation is twice the size of the right subpopulation. 

The default unit of measure throughout this paper will be standard deviations. This unit of measure will refer to the standard deviation measurement used in the bivariate normal distributions. E.g. when this paper reports that the distance between two points is 1 standard deviation then this is equivalent to twice the standard deviation in the subpopulation distribution.

[^covarmatrix]: We further assume no correlation among the two dimensions. The correlation coefficient $\rho$ is zero and the covariance matrix is $\left[ \begin{array} 0.5^2 & 0\\ 0 & 0.5^2 \end{array} \right]$.

[^singlemarket]: _[Definition of market: ... When is it a single market and when is it two separate markets ...]_

![Distribution of consumer ideal points, contour of consumer ideal points and example of market areas and market centroids with 5 firms for $\mu$ equal to respectively 0.5 and 1.5 and $n_l/n_r$ equal 2.](Graphics/temp_dist.png)

| Rel. size | Polarisation   | Pop. x-mean   | Pop. x-stddev   | 
| $n_l/n_r$ | $\mu_r=-\mu_l$ | $\mu_{xpop}$  | $\sigma_{xpop}$ | 
|:--------- |:-------------- |:------------- | :-------------- |
| 1         | 0.5            | 0             | 0.70711         |
| 1         | 1.0            | 0             | 1.118           |
| 1         | 1.5            | 0             | 1.5811          |
| 2         | 0.5            | -0.16667      | 0.68718         |
| 2         | 1.0            | -0.33333      | 1.0672          |
| 2         | 1.5            | -0.5          | 1.5             |

_[table: population statistics]_

Given the aggregated consumer distribution one approach would be to randomly draw a finite number of consumers from the distribution. Market shares can then be calculated by counting the number of consumers. Like Laver and Sergenti (2011) this paper will not employ this approach. Instead the consumer distribution is carried through and market shares are calculated by aggregating probability mass. Similarly the centroid of the market is weighted by the probability mass rather than the number of consumers. Although the approaches will yield the same results on average, the latter is independent of particular draws, thus we save computational power not having to run repetitions to obtain the average values.


## 3.2 Agent-based modelling

### Firm behaviour

Our point of departure is that each firm chooses the location that maximises its market share, given the location of the other firms. However all firms choose their location simultaneously, and thus when a firm has to choose its own location, the location of other firms is unknown. Instead of solving for the optimal location that maximise market share, firms may use heuristics or rules of thumb when they choose location. The literature review provided an overview of the many different decision rules previously considered. This paper will use three of the heuristic decision rules laid out by Laver and Sergenti (2011) as the base to which other decision rules are compared. These rule are described in detail below. _[rational vs. rule of thumb]_
_[Why these three rules? No exploration, social optimum, and high on exploration.]_

**Sticker:**
The simplest decision rule a firm can use is the *sticker*-rule. With this decision rule the firm sticks to its initial position regardless of what happens. This could be a firm that is either unwilling to change or incapable of change. The management of the firm may have an unyielding belief in the long-run superiority of the position of the firm, discouraging it from any change, even in times of despair. Ie. a belief that the market share of the firm will always recover and excel in and of itself. The firm might also be unable to change, due to financially constraint such as the fixed cost of relocating or investments that cannot be recuperated. And finally, given the  task of predicting the future location of all other firms, the firm might see its current position as less risky than a new location based on uncertain predictions.

**Aggregator:**
Firms using the *aggregator*-rule constantly seeks to please its own customer base. The firm does not try and predict the future. Instead it takes its current market area and locates at the centre of it. More specifically the centre or *centroid* is the mean ideal point of all customers of the firm. Thus the centroid position takes into account the population density within the market area, drawing the firm towards the centre of mass.

The future landscape of competing firms may change and with it the market areas. So there is no guarantee that the current centroid is also the centroid of the future. Nonetheless firms using the *aggregator*-rule continually pursue the mean ideal point of their customer base. Likewise there is no guarantee that the relocation of the firm will increase market shares, or even maintain current shares. Market shares are in a sense a secondary priority for the firm using the *aggregator*-rule. The management of the firm may reason that they can retain and recruit new customers by pleasing their current customers[^loyalty]. 

[^loyalty]: Noted that customer loyalty is not incorporated in the models. Customers always choose the closest firm regardless of previous purchasing history. And this might understate the efficiency of firms using the *aggregator*-rule in markets with high degree of customer loyalty.

When all firms in the market use the *aggregator*-rule then the model is an implementation of the *Lloyd’s algorithm* and the location of firms will converge to the stable *Centroidal Voronoi Tessellation* (Laver and Sergenti, 2011, chapter 3 pp. 48-49). The *Centroidal Voronoi Tessellation* (CVT) is a special geometric construction where each firm is located at the centroid of its own market area. *Aggregator* firms located at the current centroid will not relocate. Hence when all firms use the *aggregator*-rule the CVT is stable -- no firm relocates. The CVT has the useful property that the distance between all customers and their closest firm is minimised. It is an optimal arrangement of firms that minimises the average distance. This will prove useful later on when comparing the social welfare of the different decision rules. The all-aggregator model will maximise our social welfare measure.

The underlying requirement for a *aggregator* firm is that it has perfect knowledge of its current customer base. The firm knows the span of its current market area and the distribution of customers within the area, such that the firm can correctly determine the mean ideal point of its customers.

The *aggregator* firm never relocates outside its current market area. But otherwise there is no limit as to how far an *aggregator* firm can move at each iteration. The firm nonetheless tends to move in relatively small steps, unless the market is extremely unstable. 

**Hunter:**
The *hunter*-rule is a trial-and-error type of decision rule. The firm continues in the same direction, if it previously proved fruitful, and otherwise the firm heads in the opposite direction. At each iterations the firms move what corresponds to 0.1 standard deviation of the population distribution. If the previous move did not increase the market share, then the firm turns around and heads in a random direction drawn from the 180 degree arc in front of it. 

Firms using the *hunter*-rule never settle down. The firm endlessly hunts higher market shares with the same speed and intensity. For a firm with this decision rule there is no comfortable threshold share of the market that suffice or slows exploration. In the trade-off between exploration and exploitation the firm always chooses the first option. 

The only information used in the decision process of a *hunter* firm is the current relative change to its market share and its current direction. No information going further into the passed is used. The firm may lack memory, or may emphasis the present to such a degree that information going further back is seen as worthless. The behaviour of a *hunter* firms is most suited for a fast evolving market with high unpredictability.

The *hunter* firm moves 0.1 standard deviations each iteration. This is the *speed of adaption*. It is beyond the scope of this paper to investigate the effects of changing the speed parameter. However Laver and Sergenti (2011, chapter 7, pp. 150-151) find evidence that a speed parameter of 0.1 standard deviations results the in largest market shares for a *hunter* firm. They argue that this speed strikes the balance between quickly reaching better locations without overshooting these location when moving around. 

-----

The above described heuristic decision rules are good first approximation on how firms might choose to locate. Especially in  a simultaneous multi-agent location model where the future location of other firms is unknown. Later on we would like to investigate how foresights affects our results. But before doing so we need to return to the deliberate process of maximising market share. That is, before we can answer how a firm locates given the *predicted* locations of competing firms, we need a new decision rule on how the firm locates given *any* location of competing firms. And none of the above mentioned decision rule take the location of competing firms into consideration. We will start with a decision rule that assumes that the other competing firms stay at their current location. Later on we will extend the decision rule so it includes location predictions and learning. In the end we will have reintroduced strategic considerations in the simultaneous location model.

Let us assume that competing firms stay at their current location. We would like to know where the firm should relocate to maximise its market share. When a firm relocates it gains new customers and loses others. Thus there is a trade-off between the market area it gains and the area that it loses. To simplify matters even further we only focus on the first effect. This problem is equivalent to a new firm entering a market and choosing the location that will maximise its share. The firm itself is not part of the existing market and the market is populated with competing firms. There is extensive research on competitive location models such as the Hoteling model and the Voronoi game, and on the geometry behind Voronoi diagrams. Yet only a couple of papers provide methods on how to find the location that maximises the market area of a firm. And the methods all concern the location of newly entering firms -- none  consider the optimal location of an existing firm relocating.  Cabello, Díaz-Báñez, Langerman, Seara and Ventura (2010) use the *reverse nearest neighbour* method to find the position that maximises the number associated points or customers. However their method requires a finite number of points, and thus this method is not applicable. We assume an infinitely continuous number of consumers by using the distribution of consumers. The remaining three papers use continuous distributions, but they assume the distribution is uniform, which also make them unsuited for our needs, since we have a mixed bivariate normal distribution. Averbakh, Berman, Kalcsics and Krass (2015) use a method that partitions the solution space into smaller regions. The partition is done in such a way that the structure of the Voronoi diagram is unaffected by how the firm locates within each region. The structure is only affected by which of the regions the firm locates in. The partition simplifies the problem to a *search problem* over all regions. Their method uses the *Manhattan* distance metric. The partitioning also work with *Euclidian* distance metric. However it might be impossible to obtain exact solutions with this distance metric, because of the complexity that the *Euclidian* norm introduces in the object function (Averbakh, Berman, Kalcsics and Krass, 2015, pp. 409-410). The models in this paper uses the *Euclidian* distance metric. Cheong, Efrat and Har-Peled (2007) note the difficulties in finding analytical solutions to the problem and develop an algorithm that approximates the maximum of the object function. Their method finds the largest circle within each existing market area that does not contain any of the existing firms. They construct squares with lengths equal to the radius of the largest empty circles. These squares are partitioned into grids and they use a *$\epsilon$-approximation* to pick the cell in the grid that maximises the market area. As previously mentioned Cheong, Efrat and Har-Peled (2007) assume a uniform distribution of customers. When working with non-uniform distributions the largest empty circles is a poor criteria to use to narrow down the search for maximum. The last paper by Dehne, Klein and Seidel (2002) proves that if the location of neighbouring firms span a convex hull, then there exists a unique local maximum inside this convex hull. They use a Newton method to calculate these local maxima within each of the Delaunay circles (that is the smallest circles covering the triangles in the Delaunay Triangulation). In addition they check for corner solutions, ie. locating on top of existing firms. The optimal location for the new firm is then maximum of all these local maxima. The case where the neighbouring firms do not span a convex hull is left open. There is nothing that prevent neighbouring firms from locating such that they do not span a convex hull in the models of this paper, and thus this method is not suitable either. 

None of the currently existing methods are applicable to the models used in this paper. This is due to the non-uniform infinitely continuously distribution of consumers, where neighbouring firms may not form convex hulls, and because we use the Euclidian distance metric. Nonetheless I draw on the commonalities of these papers when constructing the decision rule that explicitly tries to find the location that maximise the market share of the firm.

![Example with five competing firms (blue markers). The Voronoi diagram illustrating the market areas of the competing firms. The Delaunay Triangulation of the competing firms and the boundaries. Red marker is the centroid of the triangle with the largest share of customers indicating the ideal position of the firm. The Voronoi diagram when including the ideal position of the firm.](Graphics/temp_maxcov.png)

**Maxcov:**
Firms using the *maxcov* decision rule aim for the optimal location that will maximise the number of customers. The firm assumes that the ideal location lies in the gaps between competing firms. The firm considers all the gaps and picks the one with the most consumers. Each gap is a triangle in the Delaunay Triangulation. The Delaunay Triangulation is constructed using the competing firms and it includes the boundary points. The latter insures that the firm can also locate outside the area spanned by competing firms (Cheong, Efrat and Har-Peled 2007 p. 556). The triangle with most consumers is selected, and the ideal location is the mean ideal point of all customers within that triangle.[^mostcustomers] 

[^mostcustomers]: _[The triangle with the most consumers is a proxy of the actual market share obtained. The consumers that the firm attracts will not necessarily be all the ones in the triangle. Therefore the triangle with most consumers may not always be the triangle that gives the firm the largest market share. However the method is correct 40-90% of the cases depending on the parameter settings and the number of firms. If the triangle was randomly selected which the triangle would only be correct in 4-16% of all cases.]_

The *maxcov* firm does not move directly to the ideal position, but moves 0.1 standard deviations in the direction. This gradual adjustment is chosen for two reasons: first, it makes the speed at which the *maxcov* firm moves comparable to the speed of the other decision rules. The *hunter* firm moves 0.1 standard deviations, and the *aggregator* does not move outside its current market area. _[Secondly, the ideal position of the firm is sensitive to changes in the position of competing firms. Even minor changes to the position of competing firms can change which triangle contains the most customers, and thus significantly change the ideal position. The gradual adjustments counteracts the sensitivity.]_

When the *maxcov* firm chooses its location it explicitly assumes that competing firms remain at their current position. _[The *maxcov* firm faces the same problem: when choosing location the location of the other firms is unknown. Not knowing how the other firms will move and without any predictions it may be reasonable to use the current position of the firm as the best point of reference.]_

In determining its location the *maxcov* firm uses information on the location of competing firms. Additionally I assume that the firm has perfect knowledge of the consumer distribution such that it can determine which of the gaps contain the largest number of consumers[^knowdist].

[^knowdist]: Alternatively -- and perhaps more realistically -- the firm could approximate which gaps contained the largest number of customers by using the market share of the surrounding firms (Fowler and Laver, 2008 p.75). However to avoid effects arising from approximation I assume the *maxcov* firm have perfect knowledge of the distribution of customers.


## 3.3 Markov Chain Monte Carlo

_[convergence / burn in]_
_[How the initial position of firms are drawn.]_
_[(Visually show the movement / iterations over time to the reader)]_
_[runs, repetitions, iterations.]_


### Monte Carlo parameterisation

_[Grid sweep vs Monte Carlo parameterisation]_




### Summary variables

There are three main perspectives we want to analyses in the competitive location model. One perspective is the location of the firms. Do firms agglomerate or cluster at particular locations or do firms disperse across the market space? Another perspective is the competitive environment. Is competition a *winner-take-all* game in which one firms is able to capture a predominant share of customers, or are customers evenly shared among firms? The last perspective concerns *social welfare*. How does the competition among firms affect the wellbeing of customers? To investigate these perspectives we construct three summary variables. These variables aggregate the state of the market into a single measure that is comparable across parameter settings and models.

**Mean Eccentricity:**
To summarise the position of firms we use the mean eccentricity. Eccentricity measures the distance from a firm to the mean ideal point of all customers. Eccentricity has the desired properties of a summary variable. It is a single measure as opposed to the coordinates of the firm that are two dimensional. It is a relative measure that is naturally interpretable as the distance to the population centre. And since we use the mean ideal point which changes with the parameters, eccentricity is comparable across different parameter settings. Had we used the origin of the coordinate system instead, this would not change with the parameters and our distance would thus dependent on the specific parameter setting. In addition the origin is an arbitrary location and thus not easily interpretable. We take the average of all firms to get the mean eccentricity in each stage.

**Effective number of firms (ENP):**
I use a relative measure of the market shares of the firms to summarise the competitive environment. The measure is known as the *effective number of parties* (ENP), but will in this paper be referred to as the *effective number of firms*. It measures the concentration of market shares among firms. The measure goes from 1 and up to the number of firms in the market (N). In a market with four firms where all firms have an equal share of the market the effective number of firms is four (ENP=N), while the effective number of firms is one (ENP=1) if a single firm captures the entire market. The ENP tells how many firms would be in the market if they all had identical shares.

$$ENP = \frac{{\left( \sum\limits_j^N s_j \right)^2}}{{\sum\limits_j^N s_j^2 }}$$

The ENP is the inverse of the Herfindahl-Hirschman Index (Laakso and Taagepera 1979, p.4). The ENP and the index measures the same thing. However the ENP is the easiest to interpret across our parameter settings where we change the number of firms.

**Mean Representation:**
It is straightforward to create a measure that summarises the consumer welfare given our utility function (see equation _[##]_). By taking the average over all customers we get the mean utility or mean representation in every stage.


# 4. ANALYSIS

## 4.1 Baseline model and decision rules

We are now ready to investigate the first models. We start with the all-sticker, all-aggregator, all-hunter and all-maxcov models. In each of these models all the firms use the same decision rule. We compare the decision rules by comparing the results across models. We start with a symmetric unimodal distribution of customers. There is no polarisation of the ideal point of the two subpopulations and the subpopulations are equally large ($\mu=0$ and $n_l/n_r=1$). Thus there is the only one free parameter of the models; the number of firms $N$. We use the grid sweep method to set the parameter value.

![Mean eccentricity for respectively all-sticker, all-aggregator, all-hunter and all-maxcov model. Where $\mu=0$ and $n_l/n_r=1$. Bands indicate +/- standard deviation.](Graphics/fig21a.png)

In the all-sticker model the firms remain at their initial position. They never relocate. Figure _[##]_ plots the mean eccentricity for each of the models. As expected we see that the average distance to the population centre is 1.5 standard deviation in the all-sticker model. This result reflects how the initial positions of firms are drawn. The initial position of firms are drawn uniformly from a circle with radius of 3 standard deviations and centre at (0,0). And thus we find that on average a *sticker* firm will be located 1.5 standard deviations from the population centre -- irrespectively of the number of firms in the market.

In the three remaining models mean eccentricity is significantly lower. The initial position of the firms in these models are drawn from exactly the same distribution, but the firms clearly moved towards the population centre. 

In the all-aggregator model the firms aim to please their current customer base. The firms choose to locate 0.4-0.8 standard deviations away from the population centre with the distance increasing with number of firms in the market. In the all-hunter model, where all firms constantly seek higher market shares, the firms choose to locate in closer proximity to the population centre than in the all-aggregator model. Hunter firms locate around 0.2 standard deviation closer to the centre than aggregator firms.

Despite the fact that the population centre has the largest density of consumers of any point in the market, firms consistently locate at a distance to this point. Even *hunter* firms that constantly seek larger market shares. This suggests that locating too close to the population centre is suboptimal (Laver and Sergenti 2011 chapter 5). By moving closer to the population centre the firm gains high density areas, but the losses clearly outweighs this gain. _[A firm that attempts to wedge its market area closer to the centre will often find as a result that the far end of its wedged market area becomes increasingly thinner. This is the lost market area.]_ 

_[Furthermore the long run dynamics come into play / vote share maximum ... ]_
_[fig median spline: vote share vs. mean eccentricity]_

_[Compare results to Eaton and Lipsey (1975)]_

In the all-maxcov model the firms deliberately attempt to maximise their market share. The firms do not rely on heuristics. And the firm take into account the position of competing firms, although assuming that competing firms do not relocate. Firms in the all-maxcov and all-hunter model locate at similar distance to the population centre. The main exception is in the market with two firms. In this case the *maxcov* firms locate at the same distance as *aggregator* firms. In the all-maxcov model there is only a slight tendency for firms to locate further away from the population centre as the number of firms increase. On average the firms locate 0.4-0.5 standard deviations away from the population centre.

![Effective number of firms (ENP) for respectively all-sticker, all-aggregator, all-hunter and all-maxcov model. Where $\mu=0$ and $n_l/n_r=1$. Bands indicate +/- standard deviation.](Graphics/fig22a.png)

In the models the actual number of firms in the market is an endogenous variable. Our experimental design allows us to investigate the models when there is anywhere from 2 to 12 firms in the market. To analyses the competitive environment we use a measure call the effective number of firms (ENP). The ENP takes into account the relative size of the firms -- that is their share of the market. The ENP tells how many firms would be in the market if they all had identical shares of the market. If the actual number of firms and the effective number of firms is the same (thin 45 degree line in figure _[##]_), then the firms in the market will have split the market evenly. If the effective number of firms is less than the actual number, then one or several of the firms will have a disproportionate share of the market.

The effective number of firms is low in the all-sticker model, ranging from around 1.5 and up to 4.5. The initial position of firms gives some firms a clear advantage over the other firms in the market. And since *sticker* firms do not relocate the uneven concentration of customers persists.

In the last three models the market is fairly evenly split among the firms. This is the result of homogenous firms competing among themselves. Since all firms use the same decision rule, no one firm has a long-run advantage over its competitors. And thus the firms end up with approximately the same long-run share of the market. More firms in the market make it increasingly harder to maintain a perfectly even split among the firms. And we see that the ENP is slightly below the actual number of firms in the all-hunter, all-aggregator and all-maxcov model when there are many firms in the market.

![Mean representation for respectively all-sticker, all-aggregator, all-hunter and all-maxcov model. Where $\mu=0$ and $n_l/n_r=1$. Bands indicate +/- standard deviation.](Graphics/fig23a.png)

We use mean representation to measure the satisfaction of the customers. This social welfare measure calculates the average utility of all customers. The utility of the customer increases when the distance to the closest firm decreases. From earlier we known that the all-aggregator model will result in firm locating along a *Centroidal Voronoi Tessellation* (CVT). The CVT minimises the average distance of all customers and therefore maximises our social welfare measure. The all-aggregator model provide a benchmark, since the location of firms is socially optimal.

In both the all-hunter and all-maxcov model where firms aim to maximise their market share, they manage to achieve almost the same mean representation as in the all-aggregator model, where firms actively aim to please their customer base. Unsurprisingly the all-sticker model scores lowest on our social welfare measure, since this model has the largest customer to firm distances. 

_[Although Núñez and Scarsini (2014) investigates a discrete competitive location model we find similar results. Namely that as the number of firms increase the location of firms converges to the distribution of consumers’ preferences. In this model we see that as the number of firms in the market increases the mean representation converges towards the social optimal level.]_


## 4.2 Asymmetric and multimodal population distribution

In the section above we got a sense of the different models and how firms react when using different decision rules. Our results have so far assumed a symmetric unimodal distribution of consumers, where there was no disagreement over the average ideal point along any of the two dimensions. There was a single peak in the distribution where the density of consumers was greater than any other point. Despite this firms generally choose to locate at a distance to this point. The literature review showed that different distribution of customer have had significant impact on the results of previous competitive location models. We now want to investigate how firms locate when there is not a single peak in the distribution? How the dynamics change when the subpopulations disagree over the average ideal points? And whether our results generalise to other distributions, such as asymmetric and multimodal distributions? 

In the following models there are three free parameters; The number of firms in the market $N$ which takes integer values between 2 and 12. The polarisation of the subpopulations $\mu$ which can take any value between 0 and 1.5. And finally the relative size of the subpopulation $n_l/n_r$ which takes any value between 1 and 2. To parameterise our models we use the Monte Carlo parameterisation method. For each run of the model the parameter values are uniformly random drawn from their respective ranges. And we run the models repeatedly to map out the entire parameter space.

In each model there is still just one decision rule present. Firms are competing against other firms using the same decision rule. Since firms using the *sticker*-rule do not relocate running the all-sticker model with asymmetric consumer distribution would not provide further insights. Thus we have left out the *sticker*-firms of this subsection.

![Mean eccentricity for respectively (a) all-aggregator, (b) all-hunter, and (c) all-maxcov model. Where $\mu \in [0, 1.5]$ and $n_l/n_r \in [1, 2]$.](Graphics/temp_mcp_meaneccentricity.png)

The results from the different decision rules are presented in separate panels. Three bands are used to summarise the results from different degrees of polarisation among the subpopulations. The first band summarises the results from the models where there is a low degree of polarisation ($\mu \le 0.5$). The second band is for a medium degree of polarisation ($0.5 < \mu < 1$). And the last band is for a high degree of polarisation ($\mu \ge 1$).

The firms using the *hunter*-rule still locate in closer proximity to the population centre than the firms using the *aggregator*-rule for any level of polarisation. However the differences in proximity between *hunter*-firms and *aggregator*-firms is much less pronounced in markets with a high degree of polarisation and many firms. Here the average distance to the population centre is around 1.2-1.4 standard deviations for both type of decision rules.

In the highly polarised setting we see significant difference between the market with 4 or more *hunter*-firms and then the market with 2 or 3 *hunter*-firms. For one we see that the average distance to the population centre increases swiftly when going from two to three, and three to four firms. While going beyond four firms in the market has negligible effect on the average distance to the population centre. This is due to firms separating when there is four or more firms in the market. Here we typically see that the firms split into two crowds. With each crowd of firms fiercely competing for the customers in the respective subpopulation. In the model with a symmetric distribution of customers the *hunter* firms choose to locate around the peak of the customer distribution. Similar behaviour is observed in the models with a high degree of polarisation among the subpopulation. However these models have two peaks which the firms tend to locate around. Firms locate close to the centre of one of the subpopulations, rather than in between the two subpopulations. The firm loses market shares when it moves too far away from the centre of a subpopulation. Thus this acts as a punishment for the *hunter* firm discouraging it from locating between the subpopulations and close to the average ideal point of all customers. On the other hand with only two *hunter* firms in the market, the firms tend to locate in between the two subpopulations. Thus they locate close to the average ideal point of all customers, but simultaneously they locate in an area with a low density of consumers. Later we show that this has great impact on the average utility of consumers. A highly polarised market with two firms solely focused on increasing their market shares, leads the firms to locate in a position where fierce competition is present. That is the firms cluster and compete for the same locations, rather than disperse across the market space. Note that this behaviour is a combination of the polarisation and the hunt for market shares. The other decision rules also show that firms cluster when there is little to no polarisation, but only the *hunter* firms maintain this clustering when the polarisation among the subpopulations increases. The effect depends upon the interplay between decision rule and population distribution.

Laver and Sergenti (2011, chapter 5) discovered the change when going from 2-3 *hunter* firms and into the realm of four or more *hunter* firms. Yet they seem unaware of an earlier and related discovery by Eaton and Lipsey (1975). Eaton and Lipsey (1975) also analyse asymmetric distributions admittedly in a slightly different setting, namely the bounded one-dimensional space (line market). They show analytically that an equilibrium only exists if the number of firms is less or equal to twice the number of modes in the density distribution, ie. $N \le 2M$ where $M$ is the number of modes in the consumer density function. They find that when the number of firms is exactly twice the number of modes then the firms locate in pairs around the quantiles of the distribution. While with less firms than twice the number of modes there is some leeway as to how firms locate. Both the pairing of firms in groups of two and the $2M$ limit is due to the single dimensional. In a one-dimensional line market the only option is which side of a point you locate on (left or right). Using this and equilibrium conditions Eaton and Lipsey (1975) show the limit of $N \le 2M$. In the two-dimensional market firms can locate all the way around a point, and thus firms need not be paired. In the *hunter* model firms still crowd together although not necessarily in pairs of two. Additionally the number of modes does not create an upper limit on the number of firms -- again since firms can locate all around a point. But the number of modes does seem to influence how firms locate. There is a single mode in the model with symmetric distribution, and two modes in models with highly polarised subpopulations. Only when the number of firms is equal or greater than the number of modes do the *hunter* firms form crowds around the peaks of the distributions. When the number of firms is less than the modes, then there is leeway as to how firms locate and we see that *hunter* firms locate between the peaks of distribution.

The symmetric distribution showed that *maxcov* and *hunter* firms locate at similar distance to the population centre, except with two firms in the market. We see similar pattern with the asymmetric distribution of customers. The *maxcov* and *hunter* firms locate a similar distance to the population centre irrespectively of the polarisation of the subpopulations. The exception is when there are only a few firms in the market. With the *maxcov* decision rule there is no abrupt change when going from 2-3 firms and to 4 or more firms as with the *hunter* decision rule. The *maxcov* firms always separate, also with two or three firms in the market. The firms locate around the centre of each subpopulation when consumers are polarised. Furthermore the *maxcov* firms separate even in the case with no polarisation of the subpopulations. The two *maxcov* firms will not agglomerate at the population centre but instead locate at a distance to the centre. This contrasts starkly with the two *hunter* firms that agglomerate around the population centre at both low and medium degrees of polarisation.

![Effective number of firms (ENP) for respectively (a) all-aggregator, (b) all-hunter, and (c) all-maxcov model. Where $\mu \in [0, 1.5]$ and $n_l/n_r \in [1, 2]$.](Graphics/temp_mcp_enp.png)

The asymmetric distribution reiterates the conclusions of the symmetric distribution regarding the effective number of firms (ENP). Both the model with *aggregator* and *hunter* firms show that the market is relatively even split among the firms in the market. When there are many firms then the ENP is slightly lower in the *aggregator* model than the *hunter* model. However this difference completely disappears as the polarisation of the subpopulations increase. With many firms the centre of the symmetric distribution easily overcrowds. In the *aggregator* model where firms locate to please their current customer base overcrowding leads to firms locating on different orbits around the centre. Firms located on the inner orbits attract a larger share of the customers than the firms located on the orbits further away from the centre. Polarisation of the subpopulations spreads the ideal points over a greater area which in turn reduce the overcrowding of firms. This is why in the *aggregator* model with many firms the ENP increases when going from low polarisation to medium or high degree of polarisation.

The market is also relatively even split among the firms in the *maxcov* model. When there are many firms then a high degree of polarisation results in a slightly lower ENP. As earlier noted the firms split into two crowds in the *maxcov* model. The crowd competes for the customers in each subpopulation. However in the *maxcov* model the number of firms in each crowd is not necessarily the same. It often happens that a minority of the firms manage to capture one-half of the market and this results in the lower ENP. The ENP would be 3.6 if a single firm out of 12 firms captured half the market[^ENPcalculation]. However with 12 firms in the market we observe an ENP around 10. This tells us that a single firm is unable to capture and maintain half the market by itself in the long-run -- it has to be a group of firms.

[^ENPcalculation]: With N=12 and the market split in two halves the ENP is $\frac {(0.5+0.5)^2}{\left(\frac{0.5}{11}\right)^2 \times 11 + 0.5^2} = 3.6$ when 1 firm has 50% of market alone and the 11 other firms share the remaining 50% equally. 

![Mean representation for respectively (a) all-aggregator, (b) all-hunter, and (c) all-maxcov model. Where $\mu \in [0, 1.5]$ and $n_l/n_r \in [1, 2]$.](Graphics/temp_mcp_meanrepresentation.png)

The *aggregator* model also results in the CVT when the distribution of consumers is asymmetric. And thus this model maximises our social welfare measure -- the mean representation. Once again we can use the *aggregator* model as a benchmark. The difference between the mean representation in the *aggregator* model and the *maxcov* model is minuscule.

In the *hunter* model we once again see that the asymmetric distribution of consumers give rise to significant changes when going from 2-3 firms and to 4 or more firms. This is particularly pronounced in the models with a high degree of polarisation. With two or three *hunter* firms the mean representation is around -2, while with four or more firms the mean representation is between -0.5 and -0.2. As earlier noted this is the result of *hunter* firms locating in between the centres of the subpopulations when there are 2-3 firms in the market. Although the firms locate close the average ideal point of all customers they also locate in an area with a low density of consumers. Because firms locate far from densely populated areas it significantly reduces the average utility of customers. On the other hand firms separating into crowds that locate around the centres of the subpopulation when there are four or more firms in the market. Laver and Sergenti (2011, chapter 5, p. 102) refer to this as the *“sea change”* in mean representation when reaching four or more firms.

We are now at the point where we have an understanding of the baseline decision rules; *sticker*, *aggregator*, *hunter* and *maxcov*. The first three rules rely on heuristics, while the last rule deliberately and directly maximises the market share of the firm. We know how firms that use these rules choose to locate -- both when the distribution of consumers is symmetric and when it is asymmetric. We know which type of competitive environment arises from the location behaviour of the firms. And we know how this behaviour affects the overall welfare of consumers. The following section looks at how firms locate when they take the predicted location of competing firms into considerations.

## 4.3 Decision rules with foresight

When a firm chooses its own location, the location of the other competing firms is unknown. The firm may try to predict the location of the other firms. However if multiple firms use this approach then the location outcome that each firm is trying to predict will depend on predictions that the firm and the other firms form. As Arthur (2014, p.175) writes *”predictions are forming a world those predictions are trying to forecast”*. This self-referential loop leads to logical indeterminacy. _[...]_ Because of this logical indeterminacy the maximisation problem of the firm is ill defined and cannot be solved by means of deductive reasoning. 

_[ ... Simultaneous ... heterogeneous agents ... infinite regress ... Note that the lack of a solution to the maximisation problem is not the result of bounded rationality, ie. mental capacity of the agents need to calculate the correct solution. We have make no such restriction. Agents have unbounded rationality. ... ]_

The two decision rules discussed in this section use inductive rationality. The firm holds several hypotheses and uses these to make predictions on how the other firms will locate. A hypothesis consists of a proposition that might not hold true and so contrary evidence weakens the hypothesis. The firm tests its hypotheses by comparing the predicted location of the other firms to the observed locations. Thereby the firm learns which hypotheses are plausible and thus applicable moving forward. Predictions are made using the hypothesis that worked best in the past. The firm locates -- like the *maxcov* firm -- such that it maximises its market share, but uses the predicted location of all competing firms rather than their current location. In the first decision rule the firm is endowed with a set of hypotheses. These hypotheses are exogenously given and do not change over time. Only the accuracy of each hypothesis changes in pace with its predictions being tested. I refer to this decision rule as *maxcov-inductor* or simply *inductor*, since the firm uses inductive rationality. The second decision rule is an expansion of the *maxcov-inductor* rule, but the firm gradually discards poorly performing hypotheses and forms new hypotheses. If possible new hypotheses should perform at par or better than the existing hypotheses. Therefore new hypotheses are formed through an evolutionary process that mutates and fuses the best existing hypotheses. Replacing old hypothesis is another way in which learning takes place -- leading firms to make better predictions. Hypotheses are endogenous in this decision rule. I refer to this rule as the *maxcov-inductor-GA* or *inductor-GA*, since a genetic algorithm generates the new hypotheses. I use two decision rules such that I can separate the effects from respectively inductive reasoning and endogenous hypotheses. With two decision rules these effects can be analysed in turn.

The following method is a modified version of the method first developed for *The Santa Fe Institute Artificial Stock Market Model* and described by Arthur (2014, chapter 3) and Arthur, Holland, LeBaron, Palmer and Tayler (1996). In the stock market model multiple agents try to predict the stock price. Each agent faces one unknown factor. The agent’s demand for shares depends on the agent’s predicted stock price, ie. the agent’s action depends on a single prediction. And the actual stock price rely upon the aggregated demand of all agents. In this paper an agent or a firm attempts to predict the behaviour of all other firms. Each firm faces $N-1$ unknown factors. The firm locates based on its predicted location of competing firms, ie. the action of the firm depends on multiple predictions. Thus the most significant modification of the method is going from a many-to-one prediction to a many-to-many prediction setting. Additionally the stock price is one-dimensional -- it can go up or down. Whereas the position of the firm is two-dimensional. The firm can relocate in any 360 degree direction. This however only requires a slight modification to the forecasting model used in the method.

_[ ... Is it possible to adequately describe the state of all competing firms using a single measure, rather than one measure for each competing firm. Possible something based on the Delaunay triangulation/graph (graph properties such as eccentricity/radius, circumference/geodesic, diameter, degree) ... ]_

**Maxcov-Inductor:**
A firm with the *maxcov-inductor* decision rule maintains several hypotheses on how competing firms locate. The firm uses the hypothesis that fits the current state and that previously proved most accurate to forecast the future location of a competing firm. When the firm chooses its own location it relies on the predicted location of all competing firms.

The firm is endowed with $M$ number of hypotheses. While each hypothesis might only be relevant to a narrow set of situations, together the array of hypotheses cover a wide range of different situations. At every iteration the firm only considers the hypotheses specific to the current state and ignores the remaining hypotheses. This makes the firm capable of “recognising” different situations and applying the appropriate forecast.

Each hypothesis consists of two parts jointly forming a *condition/forecast* rule. The condition part specifies which situations trigger the forecast. And the forecast part contains the specific estimates used to make a prediction about the future location.

To describe the current state we use a 13-bit descriptor. The descriptor $J_j$ summarises the location behaviour of firm $j$. Eg. the fourth bit in $J_j$ relays whether or not *firm $j$ is more than 0.6 standard deviations away from the population centre*. The tenth bit in $J_j$ relays whether or not *firm $j$ position along the agreement dimension (y-axis) is above the average of the last 16 periods*. Etc. These bits take the value 1 if the state occurred, and takes the value 0 if the state is absent. The current state of firm $j$ could for instance be summarised by the following descriptor: `1110010011010`. 

We will refer to the first 5 bits as the *fundamental bits*. They relay whether or not the distance from the firm to the population centre is greater than respectively 0.1, 0.25, 0.4, 0.6, or 1.2 standard deviations. These bits measure the degree to which the location of the firm is fundamentally different from the ideal point of all consumers. Bits 6-11 are the *tendency bits*. The bits 6-8 relay whether or not the position of the firm along the disagreement dimension (x-axis) is above the average of the last respectively 4, 16 and 64 periods. And the bits 9-11 relay whether or not the position of the firm along the agreement dimension (y-axis) is above the average of the last respectively 4, 16 and 64 periods. Thus these bits measure trends in the relocation pattern of the firm. The last two bits are respectively always on and always off. These are experimental controls. By construction they contain no information about the current state, and thus they tell to what degree firms act on useless information. 

Each *condition/forecast* rule attempts to ‘recognise’ the current state. Therefore the condition consists of 13 corresponding positions each taking the value 1, 0, or #. The condition is met if the ones and zeros match the current state descriptor. The # is a wildcard character that matches either case. Eg. The condition `###1#####0###` is satisfied if the state described by the fourth bit occurred and the state described by the tenth bit did not occur. In other words the condition will match any state where *the firm $j$ is more than 0.6 standard deviations away from the population centre while its position along the agreement dimension (y-axis) is not above the average of the last 16 periods*. The condition `###1#####0###` is not satisfied if the current state descriptor is `1110010011010`, but the condition is satisfied if the current state is `1111000010010`. More ones and zeros in the *condition/forecast* rule imply that the hypothesis is more specific, while a *condition/forecast* rule with many # will match more states and is thus a more general hypothesis.

All the *condition/forecast* rules that matches the current state of firm $j$ are said to be active. Among these active *condition/forecast* rules the rule with the best accuracy is used to forecast the future location of firm $j$. In the case where several rules tie for the best accuracy one of the rules is selected randomly and used to forecast. The accuracy of the active *condition/forecast* rules is updated once all the firms relocate and the actual location of each competing firm is revealed. The accuracy measurement is based on the forecast error variance -- a lower forecast error variance imply better accuracy. The accuracy is updated using the inverse of the moving average of squared forecast errors (see details in appendix). Over time the firm learns which hypothesis work well in a given situation. Thus the continuous updating of the accuracy of the *condition/forecast* rules facilitates learning. We will refer to this as *learning through experience*.[^learning]

[^learning]: Related to the concepts of *learning by using* (Rosenberg 1982) and *learning by doing*, although not the deductive part (Arrow 1971)*.

_[... vs. using the market share/profit forecast error as accuracy measure of the c/f rule ... ]_
_[... why use this method rather than for instance neural net/machine learning method? ... easy to expand to include the evolutionary process used in the next decision rule ... clear to see which information is used to make forecasts / how firms make forecast, while the neural net is a black box (it would work, but difficult to analysis how it works) ... ]_

The firm forecasts the future location of the competing firm $j$ using a linear forecasting model. 

$$\left( \begin{array}{*{20}{c}} {{x_{t + 1,j}}}\\ {{y_{t + 1,j}}} \end{array} \right) = \left( {\begin{array}{*{20}{c}} {{C_1}}\\ {{C_2}} \end{array}} \right) + \left( {\begin{array}{*{20}{c}} {{A_1}}&{{B_1}}\\ {{A_2}}&{{B_2}} \end{array}} \right)\left( {\begin{array}{*{20}{c}} {{x_{t,j}}}\\ {{y_{t,j}}} \end{array}} \right)$$

The six parameters of this model come from the most accurate active *condition/forecast* rule and take the form ($C_1$ $C_2$ $A_1$ $B_1$ $A_2$ $B_2$) Eg. the full *condition/forecast* rule might look like `###1#########` / (0.1 0 1.2 0 0 0.5). The rule  in this example states that if *firm $j$ is more than 0.6 standard deviations away from the population centre, then the predicted location along the x-axis is 20% further right and along the y-axis 50% less north relative to the current position, and then shifted an extra 0.1 standard deviation right along the x-axis.*

The *maxcov-inductor* firm makes predictions on the future location of all competing firms. Each competing firm $j$ has a unique current state descriptor $J_j$. But the *maxcov-inductor* firm uses the same set of $M$ *condition/forecast* rules on all the competing firms. Thus the model make the assumption that the *condition/forecast* rules are not tied to any competing firm. The hypotheses of the firm are not specific to the location behaviour of a particular competing firm, but generally applicable to any competing firms that exhibit a particular location behaviour. 

The *maxcov-inductor* firm is endowed with $M$ hypotheses. All but one hypothesis is randomly generated by the following procedure. Each position in the condition is randomly set to 1 or 0 both with probability 0.1, or set to # with probability 0.8. The forecasting parameters are drawn uniformly random from distributions with mean value 0. The forecast parameters $C_1$ and $C_2$ are drawn from a uniform distribution with range [-1.5 1.5]. The parameters $A_1$ and $B_2$ are drawn from the range [-1.2 1.2]. And $A_2$ and $B_1$ are drawn from [-0.2 0.2]. The initial accuracy or forecast error variance of each *condition/forecast* rules is set to zero. The last hypothesis is the default. This *condition/forecast* rule consists of only # so it matches any state and insures that the firm always is able to make a prediction. Each of the forecast parameters for this special rule is set to the average parameter values of the other $M-1$ *condition/forecast* rules. Because of the hypotheses are randomly drawn *maxcov-inductor* firms are heterogeneous. Each *maxcov-inductor* firm has a unique set of hypotheses.

**Maxcov-Inductor-GA:**
A firm with the *maxcov-inductor-GA* decision rule behave as described above. The firm is still endowed with $M$ hypotheses, most of which are randomly generated. But every $\varphi$ iteration the firm replaces the 20% least accurate hypotheses. The new hypotheses are created using a *genetic algorithm* (GA). This algorithm mimics an evolutionary process, ie. hypotheses are developed from earlier hypotheses with randomly occurring mutations and by crossbreeding “parent” hypotheses. The genetic algorithm uses either *mutation* or *crossover* to create a the new hypothesis. The appendix contains the values, equations and specific probabilities used. Each new hypothesis requires two parent hypotheses. These parent hypotheses are randomly drawn from the set of hypotheses not discarded, ie. the 80% most accurate hypotheses. 

With mutation the new hypothesis only inherits traits from the most fit parent. The fitness is based on the accuracy and the specificity of the *condition/forecast* rule. Accounting for the specificity implies that parent rules with wider applicability are evaluated as more fit, and thus more likely to form the basis of the new hypothesis. This implies that the model will have a slight drift towards more general *condition/forecast* rules. The method mutates the condition of the parent by randomly flipping the 1, 0 and #s, and by randomly replacing or altering the forecast parameters. 

With crossover the new *condition/forecast* rule is a mix of both parents. The condition part is mixed by randomly selecting a donor parent of each of the 13 positions. Eg. The value of the first position might come from one parent, and three following positions might come from the other parent, etc. This way the 13 positions are passed on from one of the two parents, and for each position the donor parent is randomly selected. Crossover of the forecast parameter values happens by either 1) component-wise crossover of each value, 2) using the weighted average of the parents or 3) randomly picking a parent that passes on all parameter values. The method used is randomly selected, and the three methods have equal probability of being selected. In the component-wise crossover the 6 parameter values are passed on from one of the parents, and for each value the donor parent is selected randomly. The weighted average of the parents parameter values uses the accuracy as weights.

To insure that the new hypotheses have a reasonable chance of being used, each new *condition/forecast* rule inherits the average accuracy of its parents. In case the parent rules have never matched a state -- and thus never been active -- then the new hypothesis takes the median accuracy of all the non-discarded hypotheses. The firm always maintains the special *condition/forecast* rule that only consists of #. However its parameter values are updated such that these equal the weighted average of all new and non-discarded rules, where the accuracy is used as weight.

The process of discarding the poorly performing hypotheses and forming new hypotheses based on the most accurate is another way in which the firm learns. The firm learns to make better predictions by gradually refining its hypotheses. I will refer to this as *learning through adaptation*. *Maxcov-inductor-GA* firms are heterogeneous, since their initial hypotheses are randomly drawn and because new hypotheses are formed based on the unique experiences of the firm.

### Results

**Maxcov-inductor:**

![Mean eccentricity for maxcov-inductor model. Bands indicate +/- standard deviation.](Graphics/fig611a.png)

![Effective number of firms (ENP) for maxcov-inductor model. Bands indicate +/- standard deviation.](Graphics/fig612a.png)

_[Not locked into position. The average distance to the population centre is about the same. Locates closer the the average ideal point along the dimension with no disagreement (that is closer to y=0).]_
_[Mean eccentricity: no change compared to *maxcov* model.]_
_[ENP: falls since some firms are endowed with more accurate condition/forecast rules.]_


**Maxcov-inductor-GA:**

![Mean eccentricity for maxcov-inductor-GA model. Bands indicate +/- standard deviation.](Graphics/fig621a.png)

![Effective number of firms (ENP) for maxcov-inductor-GA model. Bands indicate +/- standard deviation.](Graphics/fig622a.png)

_[Mean eccentricity: When there are few firms in the market then the firms locate further from the population centre. [Closer to y=0. The location perimeter / boundary of where firms locate has along the x-axis has not changed. The firms will only locate out to a certain point/distance, that is not too far away from the centres of the subpopulations. Thus the mean eccentricity tells us that firms to a greater extend locate in between the subpopulations, and that this behaviour increases as the number of firms increase. This is why we see a decline in the mean eccentricity as the number of firms increase. This is not a stable location pattern, but instead it reflects that firms in transition -- relocating from one subpopulation to the other.]]_
_[ENP: low. The firms end up locating at the subpopulation centre.  The firms separate into two crowds: the crowds are uneven in size. => low ENP. When firms locate on top of each other ==> the firms very likely to lock into position.]_


# 5. CONCLUSION

_[decision rule with foresight]_

# A. APPENDIX 

The number of hypotheses is set to $M=100$. 

-----

To **update the accuracy** of a *condition/forecast* rule we use the inverse of the moving average of squared forecast errors. The accuracy of firm $i$ using hypothesis $m$ at iteration $t$ is:

$$e^2_{t,i,m} = \alpha_a e^2_{t-1,i,m} + (1-\alpha_a) \left( X_{t+1,j} - E_{t,i,m} [X_{t+1,j}] \right)^2, \quad \forall j \ne i$$

where $\alpha_a$ is the memory parameter and $X_{t+1,j}$ is the future location of competing firm $j$. The memory parameter is set to $\alpha_a = 1-1/75 = 74/75$.

-----

**Fitness measure** of rule $m$ at iteration $t$ for firm $i$ is:

$$f_{t,i,m} = M - e^2_{t,i,m} - Cs_m$$

Where $M$ is the number of *condition/forecast* held by firm $i$, since this is constant and identical across firms the term can be left out. $e^2_{t,i,m}$ is the forecast error variance, and $C$ is the cost levied on the specificity. And $s_m$ is the specificity of rule $m$ calculated as the number of ones and zeros in the condition part of the rule (ie. all the # are not counted).

-----

It is randomly decided if crossover or mutation is used to create the new condition/forecast rule. The crossover method is used with probability $p$, and mutation method with probability $1-p$. This paper uses $p = 0.3$.

**Mutation method:**
Each position in the condition is mutated or flipped with probability 0.03. The probability that 0 or 1 is flipped to # is 2/3. The probability that 0 is flipped to 1 and visa versa is 1/3. The probability that # is flipped to 1 or 0 is 1/3 respectively, with the remaining 1/3 probability that # is not flipped. With these flip-probabilities on average maintain the number of 1, 0 and # in the rule. Each forecast parameter value is either replaced or changes, each with probability 0.2. Leaving 0.6 probability that the parameter value is unchanged. If replaced then the new parameter value is drawn randomly from the same ranges as the initial parameter values (see _[page ##]_). If changed then the new parameter values altered with a random amount in the range plus/minus 0.5% of the initial parameter range.