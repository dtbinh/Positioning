To analyse the location behaviour of firms we construct an agent-based model[^computation] akin to Laver and Sergenti (2011). We reformulate the model in terms of firms competing for customers, when they choose their geographic location. It is simply shorter to write ‘firms’ and ‘relocate’ rather than ‘political parties’ and ‘shift policy position’ and it reads more naturally. The results of the model are still valid in a political context, as well as in the context where manufacturers competing for customers through product differentiation when choosing product characteristic along two dimensions. 

[^computation]: All models in this paper have been programmed from the button up. The code is available at https://github.com/jsekamane/Positioning/tree/master/Models/ABM. All the models are build in MATLAB (R2015a) and run on a MacBook Pro with a 2.6 GHz quad-core Intel Core i7 processor. In addition several of the models use MATLAB’s Parallel Computing Toolbox with 4 local workers taking advantage of the quad-core processor structure to process several runs in parallel.

The decision rule of firms is exogenously determined. In each iteration the decision rule of the firm determines its location. All firms choose their location simultaneously. The model executes several iteration and analyses how the interaction of the firms affect the overall location of firms. The model assumes that all firms charge the same price, and thus this paper abstracts away price competition, and only considers location differentiation. Price competition greatly increases the complexity of the model, and is therefore beyond the scope of this paper. Furthermore firms can and do change prices more frequently than they change location. Thus price competition and location competition take place on two different time scales — a fast and a slow. It remains an open question how to appropriately incorporate the different time scales into an unified iterative model.


## 3.1 Consumers

The scope of this paper is limited to one side of the market -- the firms and their decision-making behaviour. For this reason simplifying assumptions are used for the other side of the market -- the consumers and the purchasing behaviour of consumers. For one thing consumers are static. They do not shift position, they maintain preferences, they are obliques to trends, and are in no way influenced by changes in the market. Secondly consumers behave non-strategically and demand inelastically. They always purchase a single unit from the closest firm, regardless of the distance to the firm. And finally all firms are identical in the eyes of the consumer. There is no brand recognition and no loyalty -- the customer purchase from the closest firm regardless of previous purchasing history.

Every consumer has an ideal point in the market. This is the point where the consumer wants a firm to locate. All other points are second to the ideal point. And the further away from the ideal point that the firm locates, the less attractive it becomes. Consumers have *single peaked preferences*. Essentially this paper reduces consumers to being merely their ideal points. 

Viewed in terms of geography the ideal point would be the consumer's physical location and the distance is the bee-line distance to firms. Alternatively the model can be viewed in terms of product differentiation. Then the ideal point of the consumer is his or her desired product characteristic along the two dimensions. We assume *horizontal product differentiation* along both dimension, i.e. product characteristic are non-ordinal. In addition we need to assume that product characteristics are continuous and infinitely divisible.

The utility function for each consumer is a function of the distance from the consumer’s ideal point and to the position of the firm. The utility function is equivalent to a quadratic loss function. The Euclidean distance metric is used. The utility of customer $i$ is the negative squared distance between customer $i$ and the closest firm $j$:

$$ U_i(j) = -d(i,j)^2 $$

### Consumer distribution

The simplification of consumer’s behaviour allows focused attention on the behaviour of firms. As discussed in the literature review the distribution of consumers can fundamentally change the behaviour of firms. We would like to capture and analyse these changes -- in particular the effects from asymmetric and multimodal distributions. We follow the method used by Laver and Sergenti (2011) and assume two consumer subpopulations in our market. When the two subpopulations have the same mean ideal point then the aggregated distribution of consumers will be symmetric and unimodal. While subpopulations with vastly different mean ideal points and different sizes will lead to an aggregated distribution of consumers that is asymmetric and bimodal. Thus this method of using two subpopulations is able to capture several different types of aggregate distributions using only two parameters; the polarisation of subpopulation ideal points, and the relative size of the two subpopulations. 

The market is two-dimensional, so we use the two-dimensional equivalent of the *normal distribution* -- the *bivariate normal distribution*. Both subpopulations follow a bivariate normal distribution. Using different mean values in the distribution imply that the subpopulations disagree over the ideal point. Without further restrictions the subpopulations would disagree on both dimensions. To simplify the analysis, and without lose of generality, we assume that the subpopulations have a common ideal point along one dimension and only disagree over the ideal point along the other dimension. This is equivalent to rotating the market space or coordinate system until disagreement only appears along one of the dimensions, i.e. rotating the market space such that the line going through the mean ideal points of subpopulations is parallel to an axis. One can always rotate the coordinate system without loss of information, and so this simplifies our analysis without any loss of generality. This is similar to the orthogonal transformation underlying a *principal component analysis*. We start out with two dimensions or components, and end up with two new dimensions; the disagreement dimension (x-axis) and the agreement dimension (y-axis). The two subpopulations -- from now on referred to as 'left' and 'right' -- follow a *bivariate normal distribution* with mean $(-\mu,0)$ and $(\mu,0)$ respectively, and both with standard deviation $(0.5, 0.5)$.[^covarmatrix]. Where the parameter $\mu$ measures the ideal point polarisation. The parameter measuring the relative size of the left subpopulation is $n_l/n_r$. The aggregated consumer distribution is a *mixed bivariate normal distribution* with weights based on the relative size of the subpopulations (Weisstein 2002b, Balakrishnan and Lai 2009):

$$f(x,y) = \frac{n_l/n_r}{1+n_l/n_r} f^{(l)}(x,y) + \frac{1}{1+n_l/n_r} f^{(r)}(x,y), \quad f^{(i)}(x,y) = \frac{e^{-\frac{(x-\mu_{x,i})^2}{2(\sigma_{x,i})^2} - \frac{(y-\mu_{y,i})^2}{2(\sigma_{y,i})^2}}}{2\pi\sigma_{x,i}\sigma_{y,i}}$$

The mean ideal point on the y-axis is 0 for both subpopulations hence no disagreement along this dimension. The mean ideal point on the x-axis is $-\mu$ for the left subpopulation and $\mu$ for the right subpopulation. For $\mu > 0$ there is disagreement along the x-axis. I will analysis the polarisation parameter in the range $\mu \in [0, 1.5]$ -- a range that spans both an unified and a split market. At the lower bound of the range there is no disagreement and the market consists of one unified customer base. At the upper bound there is essentially no overlap between the subpopulations and the customer base is split. The range of the relative subpopulation size is $n_l/n_r \in [1, 2]$. The subpopulations are equally large when the $n_l/n_r = 1$, while $n_l/n_r = 2$ indicates that the left subpopulation is twice the size of the right subpopulation. See figure _[##]_.

The default unit of measure throughout this paper will be standard deviations. This unit of measure will refer to the standard deviation measurement used in the bivariate normal distributions, e.g. when this paper reports that the distance between two points is 1 standard deviation, then this is equivalent to twice the standard deviation in the subpopulation distribution. The unit of measure is independent of the space and the coordinate system, making it easier to compare results with other studies.

[^covarmatrix]: That is, $(\mu_{x,l},\mu_{y,l}) = (-\mu,0)$ and $(\mu_{x,r},\mu_{y,r}) = (\mu,0)$, while the standard deviation is $(\sigma_{x,i},\sigma_{y,i}) = (0.5, 0.5)$, for $i = \{l,r\}$. We further assume no correlation among the two dimensions. The correlation coefficient $\rho$ is zero and thus the covariance matrix is $\left[ \begin{array} 0.5^2 & 0\\ 0 & 0.5^2 \end{array} \right]$.

![Example of market where $\mu$ equal to respectively 0.5 and 1.5 and $n_l/n_r$ equal 2. a) and b) Distribution of consumer ideal points. c) and d) the contour of consumers’ distribution of ideal points and the example with the location of 5 firms. e) and f) Example of market areas with 5 firms and the market centroids (weighted with the probability mass in each respective market).](Graphics/temp_dist.png)

Given the aggregated consumer distribution one approach would be to randomly draw a finite number of consumers from the distribution. Market shares can then be calculated by counting the number of consumers. This paper, like Laver and Sergenti (2011), will not employ this approach. Instead the consumer distribution is carried through and market shares are calculated by aggregating probability mass. Similarly the centroid of the market is weighted by the probability mass rather than the number of consumers (centroid shown in figure _[##]_). Although the approaches will yield the same results on average, the latter is independent of particular draws, thus we save computational power not having to execute several repetitions to obtain average values.

We will now evaluate the consumer distribution function, with particular focus on the combinations of the parameter values that give rise to a single peak in the distribution, and the combinations which give two peaks. This information will help us interpret the results of the model later on. The consumer distribution function is a mixed bivariate normal distribution. The distribution is symmetric along the y-axis, so the peaks and saddle points of the distribution will all have coordinate zero, $y^p = y^p_r = y^p_l = y^s =0$. We need only focus on determining the coordinates of the peaks and saddle points along the x-axis. Because of the symmetry and zero covariance between x and y, we can determine the x-coordinates using the marginal probability function $f_x$. This function is equivalent to a mixed univariate normal distribution (Weisstein 2002a, Weisstein 2002b, Balakrishnan and Lai 2009). We express the function in terms of the free parameters; polarisation of the ideal points of the subpopulation, $\mu$, and the relative size of the subpopulation $n_l/n_r$:

$$f_x(x,\mu,n_l/n_r) = \frac{n_l/n_r}{1+n_l/n_r} \frac{ e^{-\frac{(x+\mu)^2}{2\left(\frac{1}{2}\right)^2}}}{\sqrt{2\pi}\sqrt{\frac{1}{2}}} + \frac{1}{1+n_l/n_r} \frac{ e^{-\frac{(x-\mu)^2}{2\left(\frac{1}{2}\right)^2}}}{\sqrt{2\pi}\sqrt{\frac{1}{2}}}$$

To find the extrema of the function we differentiate with respect to $x$:

$$\frac{\partial f_x(x,\mu,n_l/n_r)}{{\partial x}}  = \frac{n_l/n_r}{1+n_l/n_r} \frac{ e^{-2(x+\mu)}}{\sqrt{\pi}}\left(-4(x+\mu)\right) + \frac{1}{1+n_l/n_r} \frac{ e^{-2(x-\mu)}}{\sqrt{\pi}}\left(-4(x-\mu)\right)$$
$$= \frac{-4e^{-2(x+\mu)} \left((x+\mu)n_l/n_r + (x-\mu)e^{8x\mu} \right) }{(1+n_l/n_r)\sqrt{\pi}}$$

Setting the derivative equal to zero and rearranging gives:

$$\frac{\partial f_x(x,\mu,n_l/n_r)}{{\partial x}} = 0 \Leftrightarrow (x+\mu)n_l/n_r + (x-\mu)e^{8x\mu} = 0$$
$$\Leftrightarrow n_l/n_r = - \frac{x-\mu}{x+\mu} e^{8x\mu}$$

It is not possible to solve for x, so instead we plot the equation in a $(\mu, x)$ diagram to evaluate the expression, see figure _[##]_. The curve splits into several branches revealing the bifurcation, i.e. for specific combinations of $\mu$ and $n_l/n_r$ our x takes on multiple values (Strogatz 1994). As $\mu \to 0$ implies $x \to 0^-$ when approaching from the negative side. However as $\mu \to 0$ and $x > 0$ we will observe a jump between $\mu \simeq 0.657$ and $\mu = \frac{1}{2}$. The bifurcation implies that all the models in this paper are non-linear.

![Bifurcation diagram. The curves indicate the extrema of the population density function along the x-axis. The left subpopulation is respectively twice as large (yellow curve), half as large (red curve) and just as large (blue curve) as the right subpopulation. The grey dotted lines indicates the mean ideal points of each subpopulation. The solid black line separates the right peak of the distribution from the saddle point at different relative sizes of the subpopulations. The dots indicate the breaking point between the unimodal distribution and the bimodal distribution.](Graphics/Bifurcation.png)

Unsurprisingly we see that with no polarisation, $\mu = 0$, the distribution is unimodal with a single peak at $x^p=0$ regardless of the relative size of the subpopulation $\left(\forall n_l/n_r\right)$. With a low degree of polarisation, $\mu \in \left(0,\frac{1}{2}\right]$, the distribution remains unimodal with a unique peak in between the origin and the mean ideal point of the left-subpopulation, $x^p \in (0,{\bar x_l})$, where the mean ideal point is ${\bar x_l} = -\mu$. The relative size of the subpopulation determines the exact value of $\mu$, for which the distribution becomes bimodal. With subpopulations of equal size, $n_l/n_r=1$, the distribution is bimodal for $\mu > \frac{1}{2}$. In addition the peaks of the distribution are located at equal distance to the origin, while the saddle point is located at the origin, $x^s = 0$. As the relative size of the subpopulation increases so does the value of $\mu$, for which the distribution becomes bimodal. To determine this breaking point, we can differentiate equation _[###]_ with respect to $x$[^break]. As the polarisation of the subpopulation, $\mu$, increases the peaks of the distribution converges towards the mean ideal points of each subpopulation, $x^p_r \to {\bar x_r} = \mu$ and $x^p_l \to {\bar x_l} = -\mu$. With an unequal sized subpopulations, $n_l/n_r > 1$, the saddle point lies slightly to the right of the origin, $x^s > 0$. The asymmetric bifurcation diagram arises, because we choose to model the left subpopulation larger than the right, by choosing the range $n_l/n_r \in [1,2]$.

[^break]: $\frac{\partial n_l/n_r}{{\partial x}} = - \frac{2 \mu e^{8x\mu} \left(4\mu^2-4x^2-1\right)}{(\mu+x)^2} = 0$ $\Leftrightarrow x_{max} = \frac{1}{2} \sqrt{4{\mu}^2-1} \Leftrightarrow \mu_{max} = \frac{1}{2} \sqrt{4x^2+1}$ where $x_{max}$ (or $\mu_{max}$) is the breaking point between the right peak and the saddle point of the distribution. By substituting $x_{max}$ into _[##]_ we get an expression for the breaking point in terms of combinations of $\mu$ and $n_l/n_r$: $n_l/n_r = - \frac{\sqrt{4{\mu}^2-1} -2\mu}{\sqrt{4{\mu}^2-1} +2\mu} e^{4\sqrt{4{\mu}^2-1}\mu}$. Further note that if $n_l/n_r$ is lower than the right-hand side of the equation, then the distribution is bimodal, and visa versa. At the upper limit of our range, when the left subpopulation is twice as large as the right ($n_l/n_r=2$), we have that $\mu \simeq 0.657$, i.e. for any $\mu > 0.657$ the distribution is bimodal. Table _[##]_ provide descriptive statistics on the consumer distribution and the coordinates for the peaks of the distribution.

{{popstat.md}}

## 3.2 Firm behaviour

We now turn our attention to the behaviour of firms. Below we present the decision rules, the underlying rationale for each rule, and the assumptions and information necessary for each rule. Only the decision rules for the baseline models are presented. Once we have analysed the model with these rules and gotten a better understanding of each rule, we will extend the model to include foresight and inductive reasoning. All firms choose their location simultaneously, and in each iteration the decision rule of the firm determines the location of the firm.

Ideally we would want each firm to choose the location that maximises its share of the market, given the location of the other firms. However all firms choose their location simultaneously, and thus when a firm has to choose its own location, the location of other firms is unknown. The firm can use the predicted location of the other firms. But the location outcome that each firm is trying to predict, depends on predictions that the firm and other firms form. This self-referential loop leads to logical indeterminacy and so the maximisation problem is ill defined. Instead of solving the maximisation problem for the optimal location, firms may use heuristics or rules of thumb, when they choose location. The literature review provided an overview of the many different decision rules previously considered. This paper will use three of the heuristic decision rules laid out by Laver and Sergenti (2011) as the base to which other decision rules are compared. We use the *sticker*, *aggregator* and *hunter*-rule since they represent respectively no exploration of the space, the social optimal and unceasing exploration for better locations.

### Sticker-rule

The simplest decision rule a firm can use is the *sticker*-rule. With this decision rule the firm sticks to its initial position regardless of what happens. This could be a firm that is either unwilling to change or incapable of change. The management of the firm may have an unyielding belief in the long-run superiority of the position of the firm, discouraging it from any change, even in times of despair, i.e. a belief that the market share of the firm will always recover and excel, in and of itself. The firm might also be unable to change, due to financially constraint such as the fixed cost of relocating or investments that cannot be recuperated. And finally, given the  task of predicting the future location of all other firms, the firm might see its current position as less risky than a new location based on uncertain predictions.

### Aggregator-rule

Firms using the *aggregator*-rule constantly seeks to please its own customer base. The firm does not try and predict the future. Instead it takes its current market area and locates at the centre of it. More specifically the centre or *centroid* is the mean ideal point of all customers of the firm. Thus the centroid position takes into account the population density within the market area, drawing the firm towards the centre of mass (see figure _[##]_).

The future landscape of competing firms may change and with it the market areas. So there is no guarantee that the current centroid is also the centroid of the future. Nonetheless firms using the *aggregator*-rule continually pursue the mean ideal point of their customer base. Likewise there is no guarantee that the relocation of the firm will increase market shares, or even maintain current shares. Market shares are in a sense a secondary priority for the firm using the *aggregator*-rule. The management of the firm may reason that they can retain and recruit new customers by pleasing their current customers[^loyalty]. 

[^loyalty]: Noted that customer loyalty is not incorporated in the models. Customers always choose the closest firm regardless of previous purchasing history. And this might understate the efficiency of firms using the *aggregator*-rule in markets with high degree of customer loyalty.

When all firms in the market use the *aggregator*-rule then the model is an implementation of the *Lloyd’s algorithm* and the location of firms will converge to the stable *Centroidal Voronoi Tessellation* (Laver and Sergenti, 2011, chapter 3 pp. 48-49). The *Centroidal Voronoi Tessellation* (CVT) is a special geometric construction where each firm is located at the centroid of its own market area. *Aggregator* firms located at the current centroid will not relocate. Hence when all firms use the *aggregator*-rule the CVT is stable -- no firm relocates. The CVT has the useful property that the distance between all customers and their closest firm is minimised. The location of firms is socially optimal when the average distance to consumers is minimised. This will prove useful later on when comparing the social welfare of the different decision rules. The all-aggregator model will maximise our measure of social welfare.

The underlying requirement for an *aggregator* firm is that it has perfect knowledge of its current customer base. The firm knows the span of its current market area and the distribution of customers within the area, such that the firm can correctly determine the mean ideal point of its customers.

The *aggregator* firm never relocates outside its current market area. But otherwise there is no limit as to how far an *aggregator* firm can move at each iteration. The firm nonetheless tends to move in relatively small steps, unless the market is extremely unstable. 

### Hunter-rule

The *hunter*-rule is a trial-and-error type of decision rule. The firm continues in the same direction, if it previously proved fruitful, and otherwise the firm heads in the opposite direction. At each iterations the firms move what corresponds to 0.1 standard deviation of the population distribution. If the previous move did not increase the market share, then the firm turns around and heads in a random direction drawn from the 180 degree arc now in front of it. 

Firms using the *hunter*-rule never settle down. The firm endlessly hunts higher market shares with the same speed and intensity. For a firm with this decision rule there is no comfortable threshold share of the market that suffice or slows exploration. In the trade-off between exploration and exploitation the firm always chooses the first option. 

The only information used in the decision process of a *hunter*-firm is the current relative change to its market share and its current direction. No information going further into the passed is used. The firm may lack memory, or may emphasis the present to such a degree that information going further back is seen as worthless. The behaviour of a *hunter*-firm is most suited for a fast evolving market with high unpredictability.

The *hunter*-firm moves 0.1 standard deviation each iteration. This is the *speed of adaption*. It is beyond the scope of this paper to investigate the effects of changing the speed parameter. However Laver and Sergenti (2011, chapter 7, pp. 150-151) find evidence that a speed parameter of 0.1 standard deviation is optimal for a *hunter*-firm. They argue that this speed strikes the balance between quickly reaching better locations without overshooting these location when moving around. 

### Deliberate reaction to competitors

The above described heuristic decision rules are good first approximation on how firms might choose to locate. Especially in  a simultaneous multi-agent location model where the future location of other firms is unknown. Later on we would like to investigate how foresights affects our results. But before doing so, we need to return to the deliberate process of maximising market share, i.e. before we can answer how the firm locates given the *predicted* locations of competing firms, we need a new decision rule that determines how the firm locates given *any* location of competing firms. None of the above mentioned decision rule take the location of competing firms into consideration. In this section we develop a decision rule that assumes that competing firms stay at their current location. Later on we extend the decision rule so it includes location predictions and learning. In the end we will have reintroduced strategic considerations into the simultaneous location model.

For now assume that competing firms stay at their current location. We want to know where the firm should relocate to maximise its market share. When a firm relocates it gains new customers and loses others. Thus there is a trade-off between the areas of the market that it gains and the areas that it loses. To simplify matters even further we only focus on the first effect. The problem is then equivalent to a new firm entering a market — populated with competing firms — and choosing the location that will maximise its share. There is extensive research on competitive location models such as the Hoteling model and the Voronoi game, and on the geometry behind Voronoi diagrams. Yet only four papers provide methods on how to find the location that maximises the market of a firm in two-dimensional space. The methods all concern the location of newly entering firms. None consider the optimal location of an existing firm that relocates. Cabello, Díaz-Báñez, Langerman, Seara and Ventura (2010) use the *reverse nearest neighbour* method to find the position that maximises the number associated points or customers. However their method requires a finite number of points. Thus this method is not applicable, since we assume an infinitely continuous number of consumers by using the distribution of consumers. The remaining three papers use continuous distributions, but they assume the distribution is uniform, which also make them unsuited for our needs, since we have a mixed bivariate normal distribution. Averbakh, Berman, Kalcsics and Krass (2015) use a method that partitions the solution space into smaller regions. The partition is done in such a way that the structure of the Voronoi diagram is unaffected by how the firm locates within each region. The structure is only affected by which of the regions the firm locates in. The partition simplifies the problem to a *search problem* over all regions. Their method uses the *Manhattan* distance metric. The partitioning also work with *Euclidian* distance metric. However it might be impossible to obtain exact solutions with this distance metric, because of the complexity that the *Euclidian* norm introduces in the object function (Averbakh, Berman, Kalcsics and Krass, 2015, pp. 409-410). The models in this paper uses the *Euclidian* distance metric. Cheong, Efrat and Har-Peled (2007) note the difficulties in finding analytical solutions to the problem and develop an algorithm that approximates the maximum of the object function. Their method finds the largest circle that is contained within each existing market area and that does not contain any of the existing firms. They construct squares with lengths equal to the radius of the largest empty circles and with the same centre as the circles. These squares are partitioned into grids and they use an *$\epsilon$-approximation* method to pick the cell in the grid that maximises the market area. As previously mentioned Cheong, Efrat and Har-Peled (2007) assume a uniform distribution of customers. When working with non-uniform distributions the largest empty circles is a poor criteria to narrow down the search for maximum, since there is no guarantee that a dominant part of the density mass falls within the circle. The last paper by Dehne, Klein and Seidel (2002) proves that if the location of neighbouring firms span a convex hull, then there exists a unique local maximum inside this convex hull. They use a Newton method to calculate these local maxima within each of the Delaunay circles (that is the smallest circles covering the triangles in the Delaunay Triangulation). In addition they check for corner solutions, i.e. locating on top of existing firms. The optimal location for the new firm is then maximum of all these local maxima. The case where the neighbouring firms do not span a convex hull is left open. And so they only provide a partial solution to the problem. There is nothing that prevent neighbouring firms from locating such that they do not span a convex hull in the models of this paper, and thus this method is not suitable either. 

None of the currently existing methods are applicable to the models used in this paper. This is due to the non-uniform infinitely continuously distribution of consumers, where neighbouring firms may not form convex hulls, and because we use the Euclidian distance metric. Since there does not exist any methods capable of finding the optimal solution, we are forced to develop a method that roughly approximates the solution. I draw on the commonalities of these papers when constructing the decision rule that explicitly tries to find the location that maximises the market share of the firm.

### Maxcov-rule

Firms using the *maxcov* decision rule aims for the location that maximises the number of customers. The firm assumes that the ideal location lies in the gaps between competing firms. The firm considers all the gaps and picks the one with the most consumers. Each gap is a triangle in the Delaunay Triangulation. The Delaunay Triangulation is constructed using only the location of competing firms, and the boundary points of the space, see figure _[##]_. The latter insures that the firm also considers locations that lie outside the area spanned by competing firms (Cheong, Efrat and Har-Peled 2007 p. 556). The triangle with most consumers is selected[^mostcustomers]. The ideal location is the weighted mean ideal point of all customers within that triangle. We use the density of consumers as weights.

[^mostcustomers]: The triangle with the most consumers is a proxy for the actual market share obtained by the firm once it locates within the triangle. The triangle and the actual market or Voronoi set (obtained once the firm locates) differs in shape and size. The triangle with most consumers may not always be the triangle that results in the highest market share for the firm. However in 40-90% of the cases the triangle with most consumers is also the triangle that results in the highest market share. If the firm did not locate in the triangle with most consumers, but select any one of the Delaunay triangles with equal probability, it would pick the triangle that results in the highest market share in only 4-16% of all cases. Although this approach is not perfect, it is sufficient and computationally fast. The problem is reduced to a *search problem* over all the $2+2(N-1)$ Delaunay triangles, where $N-1$ is the number of competing firms.

![Example with five competing firms (blue markers) in market where $\mu=0$ and $n_l/n_r = 1$. a) The Voronoi diagram illustrating the market areas of competing firms. b) The Delaunay Triangulation of competing firms and the boundary points. Red marker is the weighted centroid of the triangle with the largest share of customers. It indicates the ideal position of the firm. c) The Voronoi diagram once we include the ideal position of the firm (red dot).](Graphics/temp_maxcov.png)

Rather than move directly to the ideal position, the *maxcov*-firm, moves 0.1 standard deviations in the direction of the ideal position. There are two reasons for this gradual adjustment. First, it makes the speed at which the *maxcov*-firm moves comparable to the speed of the other decision rules. Recall that the *hunter*-firm moves 0.1 standard deviations, and  that the *aggregator* never moves outside its current market area. Secondly, the ideal position of the firm is sensitive to changes in the position of competing firms. Even minor changes may alter which of the triangles contain the most customers, and thus lead to significantly different ideal position. The gradual adjustments leads to less volatile variation in the relocation pattern of the firm, which is necessary later on when firms attempt to predict the future location of the firm.

When a *maxcov*-firm chooses its location it explicitly assumes that competing firms remain at their current position. When the firm chooses its location the location of the other firms is unknown. Not knowing how the other firms will move and without predictions it may be reasonable to use the current position of the firm as the best point of reference.

In determining its location the *maxcov* firm uses information on the location of competing firms. Additionally it is assumed that the firm has perfect knowledge of the consumer distribution such that it can determine which of the gaps contain the largest number of consumers[^knowdist].

[^knowdist]: Alternatively -- and perhaps more realistically -- the firm could approximate which gaps contained the largest number of customers by using the market share of the three surrounding firms (Fowler and Laver, 2008 p.75). However to avoid the effects arising from this approximation I assume the *maxcov*-firm have perfect knowledge of the distribution of customers.


## 3.3 Summary variables

There are three main perspectives we want to analyses in the competitive location model. One perspective is the location of the firms. Do firms agglomerate or cluster at particular locations or do firms disperse across the market space? Another perspective is the competitive environment. Is competition a *winner-take-all* game in which one firm is able to capture a predominant share of customers, or are customers evenly shared among firms? The last perspective concerns *social welfare*. How does the competition among firms affect the wellbeing of customers? To investigate these perspectives we construct three summary variables. All these variables aggregate the state of the market into a single measure that is comparable across parameter settings and models.

**Mean Eccentricity:**
To summarise the position of firms we use the mean eccentricity. Eccentricity measures the distance from a firm to the mean ideal point of all customers. Eccentricity has the desired properties of a summary variable. It is a single measure as opposed to the coordinates of the firm that are two dimensional. It is a relative measure that is naturally interpretable as the distance to the population centre. And since we use the mean ideal point which changes with the parameters, mean eccentricity is comparable across different parameter settings. Had we used the origin of the coordinate system instead, this would not change with the parameters and our distance would thus depend on the specific parameter setting. In addition the origin is an arbitrary location and thus not easily interpretable. We take the average of all firms to get the mean eccentricity in each iteration.

**Effective number of firms (ENP):**
To summarise the competitive environment we use a relative measure of the market shares of the firms. This measure is known as the *effective number of parties* (ENP), but will in this paper be referred to as the *effective number of firms*. It measures the concentration of market shares among firms. The measure goes from 1 and up to the number of firms in the market (N). In a market with four firms where all firms have an equal share of the market the effective number of firms is four (ENP=N), while the effective number of firms is one (ENP=1) if a single firm captures the entire market. The ENP tells us how many firms would be in the market, if they all had identical shares.

$$ENP = \frac{{\left( \sum\limits_j^N s_j \right)^2}}{{\sum\limits_j^N s_j^2 }}$$

The ENP is the inverse of the Herfindahl-Hirschman Index (Laakso and Taagepera 1979, p.4). ENP and the Herfindahl-Hirschman Index measure the same thing. However the ENP is the easiest to interpret across our parameter settings, since we change the number of firms in the market.

**Mean Representation:**
It is straightforward to create a measure that summarises the consumer welfare given our utility function (equation _[##]_). By taking the average over all customers we get the mean utility or mean representation in every stage. This measure tells us how well the location of firms represent the ideal point of consumers. The social optimal location of firms is the one that minimises the average distance to customers. This is equivalent to maximising mean representation.


## 3.4 Executing the model

We have now gone through the basic setup of the economic model. We have discussed the economic rationale underlying consumers and the behaviour of firms. We have specified the variables of interest and how these should be interpreted. The following section gets slightly more technical as we delve into the methodology of solving the model and estimating the variables.

The decision rule of the firm determines how the firm chooses to locate in each *iteration*. We execute the model with several *iterations*, observe how the location of firms converge, and analyse how the decision rules and parameter values of the model affect the location of firms. Figure _[##]_ provides an example of the location of firms at different iterations.

![Trajectory plots for five hunter-firms in a market with a highly asymmetric and bimodal distribution of consumers ($\mu=1.5$, $n_l/n_r=2$). The dots indicate the location of each firm after 10, 20, … 100 iterations respectively, and the lines show the location in the ten preceding iterations. The location of the firms gradually converge to a limited set of locations surrounding the mean ideal points of the two subpopulations, with a majority locating around the largest of the two. The scales for the axis are shown in first and last panel, while the black cross indicates the mean ideal point of all consumers.](Graphics/figm.png)

Analytical models are solved mathematically. Results, sensitivity to parameter changes and verification of the procedure follows directly from the derivation of key equations. Whereas computer simulated models such as agent-based models requires some pre-planning and multiple executions in order to analyse results and insure trustworthy results. This type of pre-planning resembles what takes place in an experimental study and therefore the following outline of the procedure is also known as the *experimental design*.

### 3.3.1 Model parameterisation

To initiate the models we need to set the initial position of firms. For the results of the model to be credible, they should be independent of the initial positions. The model should produce similar end-results regardless of the initial position of firms. First of, we draw the initial positions randomly[^initpos]. Secondly, we execute several *repetitions* of the model with identical parameter values, but with varying randomly selected initial positions. We compare the results across *repetitions* to insure that the location of firms converge to the same limited set of locations. This will be the case in all our models, and thus we satisfy that end-results are independent of the initial position of firms.

[^initpos]: The position of a firm is drawn uniformly random from a disc centred at (0,0) and with a 3 standard deviation radius.

We want an *experimental design* such that we can evaluate how the parameters of the model affect results. So each model is executed with different combinations of parameter values and subsequently compared. To set the parameter values we employ two different methods. In the first set of models with symmetric consumer distribution, all firms use the same decision rule, and the model only has two parameters — the decision rule and number of firms. There are 4 decision rules and the number of firms can take on 11 different values, since $N \in [2, 12]$. The parameter values are integer numbers and the entire parameter space spans 44 cells or 44 combinations, so for these models we use the simple *grid sweep* method. This method *runs* through the entire parameter space executing each combination of parameters in turn. The last set of models have two additional parameters — the polarisation of the subpopulations $\mu \in [0, 1.5]$ and the relative size of the subpopulations $n_l/n_r \in [1, 2]$. The full parameter space is huge and these parameter values are real numbers with no “natural” increments. And so there is no intrinsic reason for a grid representation to be a fair representation of the full parameter space, and even less so considering the nonlinearity of the models (see figure _[##]_). This makes the *grid sweep* method unsuitable, instead we use the *Monte Carlo parameterisation* method. For every *run* this method draws the parameter values uniformly random. With sufficiently many *runs* the method is able to map out results spanning the entire parameter space.

![Outline of the experimental design.](Graphics/ExperimentalDesign.png)

To summarise we execute several *runs* to evaluate the effect of the different parameters. In each *run* we execute several *repetitions* with the same parameter values, but with varying randomly drawn initial positions. In every *repetition* the model goes through numerous *iterations* enabling the location of firms to converge. And at each *iteration* the decision rule of the firm determines how the firm locates. See outline of the experimental design in figure _[##]_.

### 3.3.2 Markov chain

We have laid out the specifications of our model and will shortly turn to the methods used to estimate the output variables of the model. But first we take a look at the underlying process of a *run* of the model. We show that a *run* of the model constitutes a *stochastic process*. Laver and Sergenti (2011) note that most computational models can be represented by a particular *stochastic process* known as the *time-homogenous Markov chain*. We present the necessary conditions for a *time-homogenous Markov chain*. When the model satisfy these conditions we have prior knowledge of the dynamics of the process — in particular convergence and steady state. Knowing the dynamics of the process allows us to construct methods that give accurate estimates of the output variables.

#### Stochastic process

The model generates a vector with the values for all output variables at each iteration in every repetition. The values of the vector could for instance be the coordinates of the firm, the market share of firms, a measure of the effective number of firms (ENP), a measure of the eccentricity of locations and a measure of the firm’s representation of consumers’ ideal points. We follow Laver and Sergenti (2011) and use the following notation; $y_t^{(n)}$, where $y$ is the vector of values, $t$ is the iteration number and $n$ the repetition number. One repetition of the model with ten iterations would produce a series of ten vectors; $( y_1^{(1)}, y_2^{(1)}, … y_{10}^{(1)} )$. Because of the randomly drawn initial positions of firms, the exact values of these ten vectors depend on the *random seed*[^pseudorandom] and another *repetition* with a different random seed returns different values of the vectors. We can use this to our advantage, if we use a different random seed for each repetition. Then the vector $y_t^{(n)}$ represents a single realisation of a *random vector* $Y_t$, where $Y_t$ is all possible realisations of $y$ at iteration $t$. That is, $y_1^{(1)}$ is a realisation of $Y_1$ associated with repetition 1. Similarly the series of vectors $( y_1^{(1)}, y_2^{(1)}, … y_{10}^{(1)} )$ is a realisation of $( Y_1, Y_2, … Y_{10} )$ associated with repetition 1. The series of random vectors $( Y_1, Y_2, … Y_{10} )$ represents a *stochastic process*. And so, one *repetition* gives a series of vectors, e.g. $( y_1^{(1)}, y_2^{(1)}, … y_{10}^{(1)} )$. While a *run*, which consists of multiple *repetitions*, constitutes a *stochastic process*.

[^pseudorandom]: Computers are deterministic machine incapable of generating truly random numbers. Instead they use a pseudorandom number generator that approximates random numbers. The pseudorandom numbers are completely determined by the *random seed*. The random seed is the number used to initialise the pseudorandom number generator. Initiating the generator with the same random seed will produce the same sequences of random numbers. While initiating the generator with different random seeds produce different sequences of random numbers. Further note that the different sequence of numbers are independent and identically distributed (IID) across the random seeds. Although the numbers stemming from a pseudorandom number generator are not truly random, they are sufficiently random for most applications including the models in this paper.

There are three factors, that distinguish different stochastic processes. One factor is the range of all the possible values that the random vector might take. This is also known as the *state space*. Another factor is the iteration *index set*. In the example above with ten iterations the index set is $\{1, 2, … 10\}$. This paper will only focus on discrete-time processes. The last factor is the dependency between the random vectors, $Y_t$, in the process.

#### Time-homogenous Markov chain

The Markov process is a particular stochastic process, that satisfies the Markov property. The Markov property restricts the dependencies of the random vectors in the process, namely that the future state of the process may depend on the current state, but cannot depend on any of the previous states of the process. In other words the Markov process, with a vector of the state space, $X_t$, satisfies the Markov property if:

$$Prob\left[ {X_{t+1} = j} \left| {X_t = i, X_{t-1} = i_{t-1}, ... X_0 = i_0} \right. \right] = Prob\left[ {X_{t+1} = j} \left| {X_t = i} \right. \right] \quad \forall t$$

for all states of the process $i_0, ... i_{t-1}, i$, and $j$. In this paper the state of the process simply summarises the location of all firms, e.g. state $i$ refers a specific and unique location of all firms, this could be the initial position of firms. If one or more firms change location, then the process enters a different state $j$. If all firms returned to their respective initial positions, then the process would once again be in state $i$. The state space contains all possible states of the process. With a finite state space, we can write the one-step transition probability as $P_{ij}^{t,t+1} = Prob\left[ {X_{t+1} = j} \left| {X_t = i} \right. \right]$, which is the probability for $X_{t+1}$ being in state $j$, given that $X_t$ is in state $i$. A time-homogenous Markov chain further requires that the transition probabilities are independent of the iteration parameter; $P_{ij}^{t,t+1} = P_{ij}$. This is also known as a Markov chain with *stationary transition probabilities*. Only the current state affects the probability of the next state in the process and probabilities stay constant over time. Figure _[##]_ provides stylised examples of different time-homogenous Markov chains. We use matrix notation to shorten the equations describing the evolution of the process[^dimension]. We can represent all the stationary *one-step transition probabilities* using the *transition probability matrix*, $\rm P$. The *state space distribution vector*, $\pi_t$, represents the unconditional probability distribution of the state space at time $t$. Each element $i$ in the vector describes the probability that the process will be in state $i$ at iteration $t$. The *state space distribution* then evolves as given by $\pi'_{t+1} = \pi'_t \rm P$. From this equation it follows that if we know the initial state space distribution, $\pi_0$, we can derive the state space distribution vector, $\pi_t$, using $\pi'_t = \pi'_0 {\rm P}^t$.

[^dimension]: We let $s$ denote the dimension of the state space, that is the number of possible states. The transition probability matrix $\rm P$ is a $s \times s$ sized matrix. The size of the *state space distribution vector* $\pi_t$ is $s \times 1$.

![Stylised examples of time-homogenous Markov chains. Each arrow has a corresponding positive one-step transition probability, $P_{ij}$ (no arrow indicates that the one-step transition probability is zero). The process or *run* starts with an initial state space distribution $\pi_0$, where each element $i$ in the vector $\pi_0$ is the probability of the process starting in state $i$. The process converges to the stationary state space distribution, $\pi_\infty$ (states within the shaded area have positive probability, while states outside have zero probability). The random component in the all-hunter model insures that all one-step transition probabilities between different states are strictly positive. A hunter-firm never settles down and thus there are no self-loops (the diagonal in the *transition probability matrix*, $\rm P$, consists of zeroes). However the process is ergodic so it converges to a unique stationary state space distribution, $\pi_\infty$, regardless of the initial state space distribution. The all-sticker and all-aggregator model are deterministic THMC, thus all one-step transition probabilities equal 1. Both models converge to a single state rather than a distribution of states, since there is no oscillation. However the processes are non-ergodic, and thus the single state depends on the specific probabilities in initial state space distribution $\pi_0$.](Graphics/Markov2.png)

We have argued how a *run* of our model constitutes a stochastic process and linked it to the dynamics of Markov chains. We are now at a point where we can look closer at convergence and steady state. The state space distribution vector is *stationary* when $\pi_{t+1} = \pi_t$. Because of the randomly drawn initial location of firms the initial state space distribution, $\pi_0$, is seldom stationary and several iterations are need. The process reaches steady state once the state space distribution becomes stationary. That is $\lim_{t \to \infty} \pi_t = \pi_\infty$, where the stationary state space distribution, $\pi_\infty$, solves $\pi_{t+1} = \pi_t$. All time-homogenous Markov chains converge to at least one steady state distribution. A process that converges to a unique distribution vector, $\pi_\infty$, regardless of the initial distribution vector, $\pi_0$, is known as an *ergodic* process. There are two types of time-homogenous Markov chains (THMC); *stochastic THMC* that contain a random component (besides the randomly drawn initial positions), and *deterministic THMC* that do not contain any random component and where the probability that the random vector, $Y_t$, takes on a particular values is 1. This distinction is useful since not all THMC are *ergodic*, but as Laver and Sergenti (2011, chapter 4, p. 64) note all *stochastic THMC* with a finite state space are *ergodic*. With a random component in the process each state has strictly positive probability of being reached in a finite number of iterations, thus the process avoids “getting stuck” and eventually converges to the unique distribution vector (Laver and Sergeant, 2011 chapter 4, p. 71). Recall that a hunter-firm turns around and heads in a randomly selected direction. A random component such as this insures that the process is *ergodic*. We know that this process will converge to a unique state space distribution, $\pi_\infty$, and that this distribution is independent of the initial state space distribution. There is no guarantee that a *deterministic THMC* converges to a single state, since it might oscillate between several states. The deterministic process underlying the all-maxcov model — where all firms use the *maxcov* decision rule — does sometimes oscillate between several states. We will deal specifically with this special case in section _[##]_ — all other deterministic process in this paper converge to a single state. And all the *deterministic THMC* in this paper are non-ergodic. The arguments for these assertions are provided in appendix _[###]_. This means that although the process converges to a single state, this state is not unique, but depends on the initial position of firms. The method used to estimate the values of the output variables takes this into account, so our end-results are independent of the initial location of firms.

It is often possible to construct several Markov representations. When choosing the vector of the state space, $X_t$, one need to insure that the output variables, $Y_t$, can be derived from the vector of the state space, i.e. $Y_t = f(X_t)$. In addition the vector of the state space, $X_t$, has to satisfy the Markov property. In most of the models in this paper the vector of the state space only needs to contain the coordinates of the firms, since we can calculate the remaining output variables from the coordinates[^discretecoord]. Once the vector of the state space, $X_t$, reaches steady state so will the output variables, $Y_t$.

[^discretecoord]: Although the coordinates take on real numbers in theory, in practice when executed on any computer there is a limit to the precision of the coordinates. Matlab stores values using up to 64-bits ([MathWorks 2016](http://se.mathworks.com/help/matlab/matlab_prog/floating-point-numbers.html)). This limited precision is enough for us to say that the coordinates are discrete (to a high level of precision), and thus the state space is finite (although very large).

#### Estimating output variables

We calculate a mean estimate for each of the values in the output variable, i.e. we calculate the mean estimate of the effective number of firms (ENP), the mean eccentricity and the mean representation. We want an accurate estimate of the output variable in steady state, and so none of the output variables obtained in transient states can be used to estimate the steady state. We discard all *burn-in* iterations, that is the iterations needed to reach the steady state. In the subsequent section we return to the empirical issue of determining the number of *burn-in* iterations, but for now assume that the process has burnt in.

Define $\psi_t^{(n)}$ as the expected value of the output variable $Y_t$ at repetition $n$ and iteration $t$. We know that in steady state the state space distribution vector is *stationary*. Thus in steady state the expected values of any of the output variables, $Y_t, Y_{t+1}, Y_{t+2}$, et cetera, must also be stationary, $\psi_t^{(n)} = \psi_{t+1}^{(n)} = \psi_{t+2}^{(n)}$ and so forth, for repetition $n$. Removing the time subscript we have that $\psi^{(n)}$ is the expected value of $Y_t$ for repetition $n$ over all iterations $t$ in steady state. Using this we can define the process of the output variable as the sum of expected value and disturbance term, 

$$Y_t = \psi^{(n)} + \varepsilon_t$$

The disturbance term, $\varepsilon_t$, is serially correlated across iterations and therefore indexed with $t$. We execute several repetitions not just repetition $n$. For each repetition we may have a different expected value, i.e. a different value of $\psi^{(n)}$. So we model $\psi^{(n)}$ as the realised value of a random variable, where $\psi^{(n)}$ is drawn from a distribution with mean $\psi$ and standard deviations $\sigma_\psi$. Consequently $\psi$ is the expected value of $Y_t$ over all repetitions and all iterations in steady state. 

To estimate $\psi$ we can use the *ensemble average*. This entails executing several repetitions of a run, up until a pre-specified iteration $t$ that is within the steady state. And then taking the average over all repetitions of the realised values, $y_t^{(n)}$, of the output variable, $Y_t$, at iteration $t$:

$$\mbox{Ensemble Average}_t \mbox{ of } Y_t = \sum\limits_{n = 1}^N {\frac{ y_t^{(n)} }{ N }}$$

where $N$ is the total number of repetitions. The ensemble average converges to $\psi$ as the total number of repetitions go to infinity:

$$\psi = \mbox{plim} \sum\limits_{n = 1}^N {\frac{ y_t^{(n)} }{ N }}$$

The disadvantage of using the *ensemble average* to estimate $\psi$ is that we discard a lot of *burn-in* iterations. For instance if we determine that it takes 251 iterations for the process to burn in and we execute 100 repetitions. Then the first 250 *burn-in* iterations in each repetition are of no use when estimating the output variables in steady state. In total we discard 25.000 iterations and only use information from 100 iterations (the last iteration in each repetition) in our estimate. By any stretch this is a highly ineffective use of computational resources. For this reason, when possible, we would prefer to use the *time average* to estimate $\psi$. This entails executing a single repetition of a run, up until a pre-specified number of post-burn-in iteration. And then taking the average of $y_t^{(n)}$ over all post-burn-in iterations:

$$\mbox{Time Average}^{(n)} \mbox{ of } Y_t = \sum\limits_{t = 1}^T {\frac{ y_t^{(n)} }{ T }}$$

where $T$ is the total number of post-burn-in iterations and all iterations, $t \in \{1,2, … T\}$, are within the steady state. As the total number of iterations go to infinity, the time average converges to $\psi^{(n)}$, i.e. the expected value of $Y_t$ for repetition $n$. Recall that an *ergodic* process converges to the same unique state space distribution regardless of the initial distribution. And since the only difference between different repetitions within the same run is the initial positions of firms, it implies that the limiting state space distribution is independent of the particular repetition when the process is *ergodic*. So for a *ergodic* process we have that $\psi^{(n)}$ is equal to $\psi$, in which case:

$$\psi = \mbox{plim} \sum\limits_{t = 1}^T {\frac{ y_t^{(n)} }{ T }}$$

Recall the example from above where it takes 251 iterations for the process to burn in. We further assume that the process is ergodic and execute 100 post-burn-in iterations. Using the time average we execute a single repetition with an overall of 351 iterations. In total we only discard 250 *burn-in* iterations, and use information from the last 100 iterations to estimate $\psi$. In this example we reduced the number of discarded iterations by a factor 100 when using the *time average* instead of the *ensemble average*. However with a high degree of autocorrelation in the process it might not be possible to use the *time average* — which is what we show below. Further note that all our *deterministic time-homogenous Markov chains* are non-ergodic, so for these processes we have no other option, but to use the *ensemble average* to estimate $\psi$.

The disturbance term in equation [##] is independent and identically distributed (IID) across repetitions, because we use a different random seed for each repetition. As previously noted different random seeds give different sequences of numbers, and these sequences are IID, which in turn produce disturbance terms that at any given iteration are IID across repetitions. So when executing several repetitions the observed values at iteration $t$ will be reasonably equally spread around the true mean, and the *ensemble average* gives a representative mean estimate of $\psi$. The *time average* may not give a reasonably representative estimate, because the disturbance term is serially correlated across iterations. While an ergodic process will eventually map out the entire steady state distribution vector, $\pi_\infty$, the process is slow to map out the distribution if there is a high degree of autocorrelation. A fully mapped out distribution assures that the *time average* gives a representative estimate of $\psi$. To check whether enough observations have been collected to map out the steady state distribution vector we run several test repetitions. For each output variable we calculate the *R-hat statistic*[^rhat] (Laver and Sergenti 2011, Brooks and Gelman 1998). The R-hat statistic is a relative measure of the between-repetition variance and the total within-repetition variance. And thus the measure reveals whether there is a potential to trim the state space distribution further by increasing the number of iterations. A low R-hat statistic indicates less potential to trim. In the limit the R-hat statistic tends to 1. This paper uses the typical cutoff level of 1.05. If the R-hat statistic for every output variable is lower than 1.05, then we feel confident that the steady state distribution has been mapped out and proceed to using the *time average* in the final execution of the model. If the R-hat statistic is above 1.05 we can try to increase the number of iterations and re-run the test, or we will determine that the speed of convergence is prohibitively slow and resort to using the *ensemble average* in the final execution of the model. In the following section we distinguish between *stochastic THMC* where the *time average* provides a representative estimate of $\psi$ and those that do not, i.e. whether the R-hat statistic is below the cutoff level for all the output variables, or not. 

[^rhat]: The *R-hat statistics* is also known as the *potential scale reduction factor*. When running our test repetitions we calculate the *R-hat statistic* using the second half of all iterations. 

#### Determining burn in

![Example of trace plots. (a) A single repetition from a deterministic THMC. This repetition burns in after 12 iterations, where mean eccentricity flatlines. (b) A single test repetition from a stochastic THMC. Solid black line is the time-average estimate of $\psi$, calculated using the second half of all iterations. The dotted black lines indicate ± one standard deviation. This repetition burns in after 61 iterations, where the mean eccentricity first falls within one standard deviation of the estimate.](Graphics/temp_burn1.png)

We want an estimate of our output variables in steady state. So for each model we need to empirically determine the number of *burn-in* iterations. For a deterministic THMC this is fairly straightforward, since the process converges to a single state. The process has burnt in once the values no longer change, i.e. once the process becomes stationary. Empirically we set the *burn-in* period to the maximum number of iterations it takes before the output variables flatline, see figure _[##1a]_[^deterburn]. A stochastic THMC converges to a distribution of states rather than a single state (see example in figure _[##1b]_), which make it slightly more difficult to empirically determine burn in. In the processes where the *time average* provides a representative estimate of $\psi$ we first identify the runs that require most iterations to converge — so called extreme cases. This is done by executing a few repetitions from different runs (i.e. different parameter values) of the model and then visually inspecting the trace plots of the output variables[^traceplot]. Secondly, we execute several test repetitions with many iterations of these extreme cases. We focus on the extreme cases, since we want to find the number of *burn-in* iterations needed for all runs of the model to reach steady state. For each test repetition we calculate the *time average* and corresponding standard deviation using the second half of all iterations[^secondhalf]. And finally, with our estimate of $\psi$ we determine that a particular test repetition has burnt in once the output variable is within one standard deviation of the estimated $\psi$, see figure _[##1b]_. We set the *burn-in* period to the maximum number of iterations it takes for each of the test repetitions to burn in. In the stochastic THMC where the *time average* does not provide a representative estimate of $\psi$, the first two steps are unchanged; first identify runs that require most iterations to converge, and secondly execute several test repetitions. However since we cannot use the *time average* to calculate a representative estimate of $\psi$ we must rely on a less rigorous method to determine the number of *burn-in* iterations. For every test repetition we inspect the trace plots of each of the output variables to determine when the test repetition appears to have reached steady state, see example in figure _[##2]_. We set the *burn-in* period to the maximum number of iterations it takes for each of the test repetitions to burn in. In the final execution of all our models we choose to err on the side of caution and set the *burn-in* period slightly higher — often rounding up to the nearest fifty, i.e. 50, 100, 150, etc.

![Example of multiple test repetitions. To ease the inspection of multiple test repetitions I developed a small interactive tool, [*filter-time*](https://github.com/jsekamane/filter-time). The tool displays all repetition in the same trace plot, with the option to highlight a specific repetition or all repetitions with a specific combination of parameter values. As well as the option to standardise the series. These figures are derived from the tool. (a) Multiple test repetitions from different runs, and hence with different parameter values. Highlighted in red and blue are repetitions with respectively 2 and 3 firms in the market. (b) Multiple test repetitions from different runs, where the mean eccentricity is standardised around the mean of the second half of all iterations. Standardising helps when having to determine burn in, especially when considering different runs with different combination of parameter values. Clearly the repetitions with 2 and 3 firms require most iterations to burn in. This model appears to have reached steady state after approximately 150 iterations.](Graphics/temp_burn2.png)

[^deterburn]: For each repetition we find the first iteration where the value of the output variable is equal to the value at the last iteration. From all the repetitions we select the largest of these iterations. This is then the number of iterations needed for all the repetitions in the model to burn in.

[^traceplot]: A trace plot displays the iterative history of a repetition, i.e. the time series of one value of the vector $y_t^{(n)}$ by iteration, such as mean eccentricity by iterations or ENP by eccentricity etc.

[^secondhalf]: The second half procedure is a sufficient although not a necessary condition for the process to burn in (Laver and Sergenti 2011, chapter 4, page. 73). We use this procedure to get a representative estimate of $\psi$, when we have yet to determine the actual number of *burn-in* iterations.

#### Oscillation in all-maxcov model 

The all-maxcov model, where all firms use the *maxcov* decision rules is a deterministic THMC. However unlike the other discrete THMC, this process does not necessarily converge to a single state. For certain parameter values and initial locations of the firm the process oscillates between several states. If the process oscillates between many vastly different states, then calculating the ensemble-average at iteration $t$ is unlikely to provide a representative estimate of $\psi$. And because the process is non-ergodic — the distribution vector, $\pi_\infty$ depends on the initial distribution vector, $\pi_0$ — the time-average will not provide a representative estimates of $\psi$ either. Instead we assume that the process oscillates between a few states where the location of firms only differ slightly, and then calculate the ensemble average. We create a new decision rule called *maxcovrnd* with one slight modification: instead of moving 0.1 standard deviation, the firm randomly draws the speed parameter for a uniformly distribution with range $[0,0.2]$. On average the firm moves 0.1 standard deviation. But more importantly the process contains a random component, and so it constitutes a *stochastic THMC*, where we know with certainty that the ensemble average provides a representative estimate of $\psi$. When we compare the results of the all-maxcov model to the all-maxcovrnd model we find that the results are indistinguishable (see appendix _[##]_). This confirms that the deterministic process does in fact oscillates between a few slightly different states. And so we feel confident that using the ensemble-average in the all-maxcov model provides representative mean estimates of our output variable. We prefer to use the *maxcov*-rule, rather than the *maxcovrnd*-rule, since we are going to extend the model with firms that try to predict the future location of the other firms. Evaluating predictions makes less sense when the location of firms contains a stochastic component.

### Recap

![Outline of the various processes and their respective requirements. The respective methods used to estimate $\psi$ are presented furthest to the right.](Graphics/Process.png)

In sum we have constructed an experimental design that allows us to evaluate the effect of parameter changes, and insures that results are independent of the initial position of firms. We have identified the appropriate methods to estimate the summary variables. The appropriate method depend on the underlying process, see outline in figure _[##]_. Understanding the underlying process of the run provides us with prior knowledge on how we should go about solving the model. However it still requires some effort and test repetitions to determine the number of *burn-in* iterations and to determine whether the *time average* provides representative estimates. Once this has been determined the final execution of the model is straightforward.

